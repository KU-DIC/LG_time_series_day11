{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwRR_ja7_dB2"
   },
   "source": [
    "# <br>[ LG전자_DX_Intensive_Course  ] 딥러닝 기반 시계열 분석 4<br><br> : Recurrent Neural Network - GRU with Attention for Classification<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github에서 데이터 불러오기\n",
    "!git clone https://github.com/KU-DIC/LG_time_series_day11.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:25.526539Z",
     "iopub.status.busy": "2022-01-10T12:16:25.526164Z",
     "iopub.status.idle": "2022-01-10T12:16:27.200876Z",
     "shell.execute_reply": "2022-01-10T12:16:27.199845Z",
     "shell.execute_reply.started": "2022-01-10T12:16:25.526423Z"
    },
    "id": "FNO2pRll_dB_"
   },
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av4BvT6x_dCA"
   },
   "source": [
    "# <br>0. Hyperparameter Setting\n",
    "- data_dir: 데이터가 존재하는 경로 (해당 실습에서는 train/test 시계열 데이터가 존재하는 경로를 의미함)\n",
    "- batch_size: 학습 및 검증에 사용할 배치의 크기\n",
    "- num_classes: 새로운 데이터의 class 개수\n",
    "- num_epochs: 학습할 epoch 횟수\n",
    "- window_size: input의 시간 길이 (time series data에서 도출한 subsequence의 길이)\n",
    "- input_size: 변수 개수\n",
    "- hidden_size: 모델의 hidden dimension\n",
    "- num_layers: 모델의 layer 개수\n",
    "- bidirectional: 모델의 양방향성 여부\n",
    "- random_seed: reproduction을 위해 고정할 seed의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:27.203717Z",
     "iopub.status.busy": "2022-01-10T12:16:27.203402Z",
     "iopub.status.idle": "2022-01-10T12:16:27.261717Z",
     "shell.execute_reply": "2022-01-10T12:16:27.260658Z",
     "shell.execute_reply.started": "2022-01-10T12:16:27.203661Z"
    },
    "id": "8TONqm1K_dCB"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter setting\n",
    "data_dir = '/content/LG_time_series_day11/input/har-data'\n",
    "batch_size = 32\n",
    "num_classes = 6\n",
    "num_epochs = 200\n",
    "window_size = 50\n",
    "input_size = 561\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "random_seed = 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:27.265043Z",
     "iopub.status.busy": "2022-01-10T12:16:27.263770Z",
     "iopub.status.idle": "2022-01-10T12:16:27.277593Z",
     "shell.execute_reply": "2022-01-10T12:16:27.276715Z",
     "shell.execute_reply.started": "2022-01-10T12:16:27.264993Z"
    },
    "id": "XDzbMZu6_dCB"
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLMcsn2h_dCB"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUPIgDl2_dCB"
   },
   "source": [
    "#  <br>__1. Data: Human Activity Recognition Data__\n",
    "- 데이터 description\n",
    "    - Human Activity Recognition (HAR) Data는 30명의 실험자들이 각자 스마트폰을 허리에 착용하고 6가지 활동 (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying)을 수행할 때 측정된 센서 데이터로 구성된 데이터셋이다. 해당 데이터셋은 총 561개의 변수로 이루어져 있으며, 전체 데이터 중 70%는 train 데이터이고 나머지 30%는 test 데이터이다. HAR Data를 활용한 시계열 분류 task는 다변량 시계열 데이터를 input으로 받아 이를 다음 6가지 활동 중 하나의 class로 분류하는 것을 목표로 한다: 0(Walking), 1(Walking Upstairs), 2(Walking Downstairs), 3(Sitting), 4(Standing), 5(Laying). <br><br>\n",
    "\n",
    "- 변수 설명\n",
    "    - 독립변수(X): 여러 실험자에 대하여 561개의 변수를 281 시점동안 수집한 시계열 데이터 -> shape: (#실험자, 561, 281)\n",
    "    - 종속변수(Y): 시계열 데이터의 label - 0(Walking) / 1(Walking Upstairs) / 2(Walking Downstairs) / 3(Sitting) / 4(Standing) / 5(Laying) <br><br>\n",
    "\n",
    "- 데이터 출처\n",
    "    - https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:27.281701Z",
     "iopub.status.busy": "2022-01-10T12:16:27.280953Z",
     "iopub.status.idle": "2022-01-10T12:16:27.295741Z",
     "shell.execute_reply": "2022-01-10T12:16:27.294810Z",
     "shell.execute_reply.started": "2022-01-10T12:16:27.281640Z"
    },
    "id": "tqoZvscW_dCC"
   },
   "outputs": [],
   "source": [
    "def create_classification_dataset(window_size, data_dir, batch_size):\n",
    "    # data_dir에 있는 train/test 데이터 불러오기\n",
    "    x = pickle.load(open(os.path.join(data_dir, 'x_train.pkl'), 'rb'))\n",
    "    y = pickle.load(open(os.path.join(data_dir, 'state_train.pkl'), 'rb'))\n",
    "    x_test = pickle.load(open(os.path.join(data_dir, 'x_test.pkl'), 'rb'))\n",
    "    y_test = pickle.load(open(os.path.join(data_dir, 'state_test.pkl'), 'rb'))\n",
    "\n",
    "    # train data를 시간순으로 8:2의 비율로 train/validation set으로 분할\n",
    "    n_train = int(0.8 * len(x))\n",
    "    n_valid = len(x) - n_train\n",
    "    n_test = len(x_test)\n",
    "    x_train, y_train = x[:n_train], y[:n_train]\n",
    "    x_valid, y_valid = x[n_train:], y[n_train:]\n",
    "\n",
    "    # train/validation/test 데이터를 window_size 시점 길이로 분할\n",
    "    datasets = []\n",
    "    for set in [(x_train, y_train, n_train), (x_valid, y_valid, n_valid), (x_test, y_test, n_test)]:\n",
    "        T = set[0].shape[-1]\n",
    "        windows = np.split(set[0][:, :, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        windows = np.concatenate(windows, 0)\n",
    "        labels = np.split(set[1][:, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        labels = np.round(np.mean(np.concatenate(labels, 0), -1))\n",
    "        datasets.append(torch.utils.data.TensorDataset(torch.Tensor(windows), torch.Tensor(labels)))\n",
    "\n",
    "    # train/validation/test DataLoader 구축\n",
    "    trainset, validset, testset = datasets[0], datasets[1], datasets[2]\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:27.297908Z",
     "iopub.status.busy": "2022-01-10T12:16:27.297313Z",
     "iopub.status.idle": "2022-01-10T12:16:28.709754Z",
     "shell.execute_reply": "2022-01-10T12:16:28.708590Z",
     "shell.execute_reply.started": "2022-01-10T12:16:27.297834Z"
    },
    "id": "zb-hkqMy_dCC"
   },
   "outputs": [],
   "source": [
    "# Dataloader 구축\n",
    "# data shape: (batch_size x input_size x seq_len)\n",
    "train_loader, valid_loader, test_loader = create_classification_dataset(window_size, data_dir, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIyUAoms_dCD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtCMq7mX_dCD"
   },
   "source": [
    "# <br>__2. Model: GRU with Attention__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:28.711801Z",
     "iopub.status.busy": "2022-01-10T12:16:28.711455Z",
     "iopub.status.idle": "2022-01-10T12:16:28.723149Z",
     "shell.execute_reply": "2022-01-10T12:16:28.721485Z",
     "shell.execute_reply.started": "2022-01-10T12:16:28.711754Z"
    },
    "id": "cSVpidwg_dCD"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, device, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\n",
    "        # final_hidden_state.shape: (batch_size, hidden_size)\n",
    "        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)\n",
    "        batch_size, seq_len, _ = rnn_outputs.shape\n",
    "        \n",
    "        attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\n",
    "\n",
    "        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
    "\n",
    "        return attn_hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:28.725742Z",
     "iopub.status.busy": "2022-01-10T12:16:28.725395Z",
     "iopub.status.idle": "2022-01-10T12:16:28.741958Z",
     "shell.execute_reply": "2022-01-10T12:16:28.740966Z",
     "shell.execute_reply.started": "2022-01-10T12:16:28.725686Z"
    },
    "id": "sA-yGAIw_dCD"
   },
   "outputs": [],
   "source": [
    "class GRU_Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional, device):\n",
    "        super(GRU_Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attn = Attention(device, hidden_size * self.num_directions)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, seq_len = x.shape\n",
    "        \n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # initial hidden states 설정\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        rnn_output, hiddens = self.rnn(x, h0)\n",
    "        final_state = hiddens.view(self.num_layers, self.num_directions, batch_size, self.hidden_size)[-1]\n",
    "        \n",
    "        # Handle directions\n",
    "        final_hidden_state = None\n",
    "        if self.num_directions == 1:\n",
    "            final_hidden_state = final_state.squeeze(0)\n",
    "        elif self.num_directions == 2:\n",
    "            h_1, h_2 = final_state[0], final_state[1]\n",
    "            final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        # Push through attention layer\n",
    "        attn_output, attn_weights = self.attn(rnn_output, final_hidden_state)\n",
    "\n",
    "        attn_output = self.fc(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:28.744557Z",
     "iopub.status.busy": "2022-01-10T12:16:28.744120Z",
     "iopub.status.idle": "2022-01-10T12:16:32.953272Z",
     "shell.execute_reply": "2022-01-10T12:16:32.952130Z",
     "shell.execute_reply.started": "2022-01-10T12:16:28.744509Z"
    },
    "id": "m2HRSR4f_dCE",
    "outputId": "eb259f7a-1059-4434-a967-a184008ed018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_Attention(\n",
      "  (rnn): GRU(561, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (attn): Attention(\n",
      "    (concat_linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델 구축\n",
    "gru = GRU_Attention(input_size, hidden_size, num_layers, num_classes, bidirectional, device)\n",
    "gru = gru.to(device)\n",
    "print(gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjUX6jME_dCE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRqJ7cio_dCE"
   },
   "source": [
    "# <br>__3. Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:32.956085Z",
     "iopub.status.busy": "2022-01-10T12:16:32.955420Z",
     "iopub.status.idle": "2022-01-10T12:16:32.985837Z",
     "shell.execute_reply": "2022-01-10T12:16:32.979641Z",
     "shell.execute_reply.started": "2022-01-10T12:16:32.956019Z"
    },
    "id": "8JmPf7tP_dCE"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += labels.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "            epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:32.997273Z",
     "iopub.status.busy": "2022-01-10T12:16:32.996839Z",
     "iopub.status.idle": "2022-01-10T12:16:33.007777Z",
     "shell.execute_reply": "2022-01-10T12:16:33.006379Z",
     "shell.execute_reply.started": "2022-01-10T12:16:32.997225Z"
    },
    "id": "zNp8tLjv_dCF"
   },
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_loader,\n",
    "    'val': valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:33.011209Z",
     "iopub.status.busy": "2022-01-10T12:16:33.010292Z",
     "iopub.status.idle": "2022-01-10T12:16:33.020834Z",
     "shell.execute_reply": "2022-01-10T12:16:33.018863Z",
     "shell.execute_reply.started": "2022-01-10T12:16:33.011161Z"
    },
    "id": "X3uu_HF8_dCF"
   },
   "outputs": [],
   "source": [
    "# loss function 설정\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:33.025065Z",
     "iopub.status.busy": "2022-01-10T12:16:33.023813Z",
     "iopub.status.idle": "2022-01-10T12:16:44.505798Z",
     "shell.execute_reply": "2022-01-10T12:16:44.504755Z",
     "shell.execute_reply.started": "2022-01-10T12:16:33.024749Z"
    },
    "id": "aVCeY6J-_dCF",
    "outputId": "30c14be4-7ad0-44f1-eb6c-b062d3502844",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n",
      "train Loss: 1.5629 Acc: 0.3750\n",
      "val Loss: 1.2175 Acc: 0.5200\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train Loss: 1.2314 Acc: 0.4125\n",
      "val Loss: 1.1021 Acc: 0.7200\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train Loss: 1.0237 Acc: 0.7125\n",
      "val Loss: 0.9257 Acc: 0.8000\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train Loss: 0.8400 Acc: 0.7125\n",
      "val Loss: 0.7668 Acc: 0.8000\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train Loss: 0.6969 Acc: 0.7500\n",
      "val Loss: 0.7511 Acc: 0.8000\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.7500\n",
      "val Loss: 0.6344 Acc: 0.8800\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train Loss: 0.5412 Acc: 0.8375\n",
      "val Loss: 0.6572 Acc: 0.8400\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train Loss: 0.4720 Acc: 0.8750\n",
      "val Loss: 0.6236 Acc: 0.8800\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train Loss: 0.4587 Acc: 0.8500\n",
      "val Loss: 0.6472 Acc: 0.8400\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train Loss: 0.4094 Acc: 0.8875\n",
      "val Loss: 0.6241 Acc: 0.8400\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train Loss: 0.3706 Acc: 0.9000\n",
      "val Loss: 0.5147 Acc: 0.9200\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train Loss: 0.3502 Acc: 0.8875\n",
      "val Loss: 0.6149 Acc: 0.8400\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train Loss: 0.3448 Acc: 0.9125\n",
      "val Loss: 0.5956 Acc: 0.8800\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train Loss: 0.2934 Acc: 0.9250\n",
      "val Loss: 0.5908 Acc: 0.8400\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train Loss: 0.2794 Acc: 0.9250\n",
      "val Loss: 0.5087 Acc: 0.8800\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train Loss: 0.2745 Acc: 0.9125\n",
      "val Loss: 0.5963 Acc: 0.8800\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train Loss: 0.2626 Acc: 0.9250\n",
      "val Loss: 0.5921 Acc: 0.8800\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train Loss: 0.2589 Acc: 0.9250\n",
      "val Loss: 0.5923 Acc: 0.8800\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train Loss: 0.2405 Acc: 0.9375\n",
      "val Loss: 0.6329 Acc: 0.8800\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.9125\n",
      "val Loss: 0.6102 Acc: 0.8400\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.9500\n",
      "val Loss: 0.6082 Acc: 0.8400\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train Loss: 0.2147 Acc: 0.9500\n",
      "val Loss: 0.6698 Acc: 0.8400\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train Loss: 0.2146 Acc: 0.9250\n",
      "val Loss: 0.6111 Acc: 0.8400\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train Loss: 0.1847 Acc: 0.9500\n",
      "val Loss: 0.5607 Acc: 0.8800\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train Loss: 0.1874 Acc: 0.9375\n",
      "val Loss: 0.5910 Acc: 0.8800\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train Loss: 0.1658 Acc: 0.9625\n",
      "val Loss: 0.6077 Acc: 0.8800\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9625\n",
      "val Loss: 0.6335 Acc: 0.8800\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9625\n",
      "val Loss: 0.6624 Acc: 0.8800\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9625\n",
      "val Loss: 0.6816 Acc: 0.8400\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.9625\n",
      "val Loss: 0.7212 Acc: 0.8400\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9625\n",
      "val Loss: 0.6902 Acc: 0.8800\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train Loss: 0.1188 Acc: 0.9625\n",
      "val Loss: 0.7074 Acc: 0.8400\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9625\n",
      "val Loss: 0.6894 Acc: 0.8400\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.9875\n",
      "val Loss: 0.6707 Acc: 0.8400\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.9625\n",
      "val Loss: 0.6652 Acc: 0.8400\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train Loss: 0.0959 Acc: 0.9750\n",
      "val Loss: 0.9202 Acc: 0.7200\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train Loss: 0.1245 Acc: 0.9375\n",
      "val Loss: 0.6414 Acc: 0.8400\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9625\n",
      "val Loss: 0.6914 Acc: 0.8400\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train Loss: 0.1178 Acc: 0.9625\n",
      "val Loss: 0.7592 Acc: 0.8400\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train Loss: 0.0830 Acc: 0.9625\n",
      "val Loss: 0.5899 Acc: 0.8400\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train Loss: 0.1244 Acc: 0.9625\n",
      "val Loss: 0.6179 Acc: 0.8400\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9750\n",
      "val Loss: 0.8097 Acc: 0.8000\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 1.0000\n",
      "val Loss: 0.7735 Acc: 0.8400\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train Loss: 0.0567 Acc: 0.9875\n",
      "val Loss: 0.7271 Acc: 0.8400\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9625\n",
      "val Loss: 0.8096 Acc: 0.8400\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train Loss: 0.0422 Acc: 1.0000\n",
      "val Loss: 1.0101 Acc: 0.7200\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train Loss: 0.0532 Acc: 0.9750\n",
      "val Loss: 0.6227 Acc: 0.8400\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train Loss: 0.0337 Acc: 0.9875\n",
      "val Loss: 0.5918 Acc: 0.8400\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train Loss: 0.0302 Acc: 1.0000\n",
      "val Loss: 0.6041 Acc: 0.8400\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train Loss: 0.0230 Acc: 1.0000\n",
      "val Loss: 0.6308 Acc: 0.8400\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 1.0000\n",
      "val Loss: 0.7343 Acc: 0.8400\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train Loss: 0.0136 Acc: 1.0000\n",
      "val Loss: 0.8080 Acc: 0.8000\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train Loss: 0.0126 Acc: 1.0000\n",
      "val Loss: 0.8364 Acc: 0.8400\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 1.0000\n",
      "val Loss: 0.8389 Acc: 0.8400\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train Loss: 0.0102 Acc: 1.0000\n",
      "val Loss: 0.8303 Acc: 0.8400\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 1.0000\n",
      "val Loss: 0.8412 Acc: 0.8400\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train Loss: 0.0081 Acc: 1.0000\n",
      "val Loss: 0.8599 Acc: 0.8000\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 1.0000\n",
      "val Loss: 0.8673 Acc: 0.8000\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 1.0000\n",
      "val Loss: 0.8630 Acc: 0.8000\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 1.0000\n",
      "val Loss: 0.8515 Acc: 0.8000\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 1.0000\n",
      "val Loss: 0.8409 Acc: 0.8000\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 1.0000\n",
      "val Loss: 0.8443 Acc: 0.8000\n",
      "\n",
      "Epoch 63/200\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 1.0000\n",
      "val Loss: 0.8458 Acc: 0.8000\n",
      "\n",
      "Epoch 64/200\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 1.0000\n",
      "val Loss: 0.8508 Acc: 0.8000\n",
      "\n",
      "Epoch 65/200\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 1.0000\n",
      "val Loss: 0.8601 Acc: 0.8400\n",
      "\n",
      "Epoch 66/200\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 1.0000\n",
      "val Loss: 0.8738 Acc: 0.8400\n",
      "\n",
      "Epoch 67/200\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 1.0000\n",
      "val Loss: 0.8986 Acc: 0.8000\n",
      "\n",
      "Epoch 68/200\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 1.0000\n",
      "val Loss: 0.9153 Acc: 0.8000\n",
      "\n",
      "Epoch 69/200\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 1.0000\n",
      "val Loss: 0.9231 Acc: 0.8000\n",
      "\n",
      "Epoch 70/200\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 1.0000\n",
      "val Loss: 0.9329 Acc: 0.8000\n",
      "\n",
      "Epoch 71/200\n",
      "----------\n",
      "train Loss: 0.0041 Acc: 1.0000\n",
      "val Loss: 0.9355 Acc: 0.8000\n",
      "\n",
      "Epoch 72/200\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 1.0000\n",
      "val Loss: 0.9396 Acc: 0.8000\n",
      "\n",
      "Epoch 73/200\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 1.0000\n",
      "val Loss: 0.9390 Acc: 0.8000\n",
      "\n",
      "Epoch 74/200\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 1.0000\n",
      "val Loss: 0.9432 Acc: 0.8000\n",
      "\n",
      "Epoch 75/200\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 1.0000\n",
      "val Loss: 0.9459 Acc: 0.8000\n",
      "\n",
      "Epoch 76/200\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 1.0000\n",
      "val Loss: 0.9510 Acc: 0.8000\n",
      "\n",
      "Epoch 77/200\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 1.0000\n",
      "val Loss: 0.9580 Acc: 0.8000\n",
      "\n",
      "Epoch 78/200\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 1.0000\n",
      "val Loss: 0.9600 Acc: 0.8000\n",
      "\n",
      "Epoch 79/200\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.9598 Acc: 0.8000\n",
      "\n",
      "Epoch 80/200\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.9656 Acc: 0.8000\n",
      "\n",
      "Epoch 81/200\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 1.0000\n",
      "val Loss: 0.9731 Acc: 0.8000\n",
      "\n",
      "Epoch 82/200\n",
      "----------\n",
      "train Loss: 0.0030 Acc: 1.0000\n",
      "val Loss: 0.9810 Acc: 0.8000\n",
      "\n",
      "Epoch 83/200\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.9849 Acc: 0.8000\n",
      "\n",
      "Epoch 84/200\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.9910 Acc: 0.8000\n",
      "\n",
      "Epoch 85/200\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 1.0000\n",
      "val Loss: 0.9955 Acc: 0.8000\n",
      "\n",
      "Epoch 86/200\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.9974 Acc: 0.8000\n",
      "\n",
      "Epoch 87/200\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.9979 Acc: 0.8000\n",
      "\n",
      "Epoch 88/200\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 0.9983 Acc: 0.8000\n",
      "\n",
      "Epoch 89/200\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 1.0065 Acc: 0.8000\n",
      "\n",
      "Epoch 90/200\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 1.0000\n",
      "val Loss: 1.0162 Acc: 0.8000\n",
      "\n",
      "Epoch 91/200\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 1.0000\n",
      "val Loss: 1.0217 Acc: 0.8000\n",
      "\n",
      "Epoch 92/200\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 1.0000\n",
      "val Loss: 1.0233 Acc: 0.8000\n",
      "\n",
      "Epoch 93/200\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 1.0000\n",
      "val Loss: 1.0268 Acc: 0.8000\n",
      "\n",
      "Epoch 94/200\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 1.0000\n",
      "val Loss: 1.0289 Acc: 0.8000\n",
      "\n",
      "Epoch 95/200\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 1.0000\n",
      "val Loss: 1.0289 Acc: 0.8000\n",
      "\n",
      "Epoch 96/200\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 1.0000\n",
      "val Loss: 1.0326 Acc: 0.8000\n",
      "\n",
      "Epoch 97/200\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 1.0000\n",
      "val Loss: 1.0366 Acc: 0.8000\n",
      "\n",
      "Epoch 98/200\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 1.0000\n",
      "val Loss: 1.0414 Acc: 0.8000\n",
      "\n",
      "Epoch 99/200\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 1.0000\n",
      "val Loss: 1.0454 Acc: 0.8000\n",
      "\n",
      "Epoch 100/200\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 1.0000\n",
      "val Loss: 1.0509 Acc: 0.8000\n",
      "\n",
      "Epoch 101/200\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 1.0520 Acc: 0.8000\n",
      "\n",
      "Epoch 102/200\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 1.0539 Acc: 0.8000\n",
      "\n",
      "Epoch 103/200\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 1.0577 Acc: 0.8000\n",
      "\n",
      "Epoch 104/200\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 1.0587 Acc: 0.8000\n",
      "\n",
      "Epoch 105/200\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 1.0646 Acc: 0.8000\n",
      "\n",
      "Epoch 106/200\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 1.0670 Acc: 0.8000\n",
      "\n",
      "Epoch 107/200\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 1.0000\n",
      "val Loss: 1.0755 Acc: 0.8000\n",
      "\n",
      "Epoch 108/200\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 1.0000\n",
      "val Loss: 1.0814 Acc: 0.8000\n",
      "\n",
      "Epoch 109/200\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 1.0000\n",
      "val Loss: 1.0844 Acc: 0.8000\n",
      "\n",
      "Epoch 110/200\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 1.0000\n",
      "val Loss: 1.0818 Acc: 0.8000\n",
      "\n",
      "Epoch 111/200\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 1.0802 Acc: 0.8000\n",
      "\n",
      "Epoch 112/200\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 1.0770 Acc: 0.8000\n",
      "\n",
      "Epoch 113/200\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 1.0790 Acc: 0.8000\n",
      "\n",
      "Epoch 114/200\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 1.0821 Acc: 0.8000\n",
      "\n",
      "Epoch 115/200\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 1.0882 Acc: 0.8000\n",
      "\n",
      "Epoch 116/200\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 1.0963 Acc: 0.8000\n",
      "\n",
      "Epoch 117/200\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 1.1050 Acc: 0.8000\n",
      "\n",
      "Epoch 118/200\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 1.1081 Acc: 0.8000\n",
      "\n",
      "Epoch 119/200\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 1.1115 Acc: 0.8000\n",
      "\n",
      "Epoch 120/200\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 1.1150 Acc: 0.8000\n",
      "\n",
      "Epoch 121/200\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 1.1173 Acc: 0.8000\n",
      "\n",
      "Epoch 122/200\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 1.1229 Acc: 0.8000\n",
      "\n",
      "Epoch 123/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1258 Acc: 0.8000\n",
      "\n",
      "Epoch 124/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1285 Acc: 0.8000\n",
      "\n",
      "Epoch 125/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1298 Acc: 0.8000\n",
      "\n",
      "Epoch 126/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1322 Acc: 0.8000\n",
      "\n",
      "Epoch 127/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1325 Acc: 0.8000\n",
      "\n",
      "Epoch 128/200\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 1.1334 Acc: 0.8000\n",
      "\n",
      "Epoch 129/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1353 Acc: 0.8000\n",
      "\n",
      "Epoch 130/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1406 Acc: 0.8000\n",
      "\n",
      "Epoch 131/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1454 Acc: 0.8000\n",
      "\n",
      "Epoch 132/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1526 Acc: 0.8000\n",
      "\n",
      "Epoch 133/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1575 Acc: 0.8000\n",
      "\n",
      "Epoch 134/200\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 1.1645 Acc: 0.8000\n",
      "\n",
      "Epoch 135/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1705 Acc: 0.8000\n",
      "\n",
      "Epoch 136/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1721 Acc: 0.8000\n",
      "\n",
      "Epoch 137/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1718 Acc: 0.8000\n",
      "\n",
      "Epoch 138/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1698 Acc: 0.8000\n",
      "\n",
      "Epoch 139/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1702 Acc: 0.8000\n",
      "\n",
      "Epoch 140/200\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 1.1722 Acc: 0.8000\n",
      "\n",
      "Epoch 141/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1769 Acc: 0.8000\n",
      "\n",
      "Epoch 142/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1800 Acc: 0.8000\n",
      "\n",
      "Epoch 143/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1797 Acc: 0.8000\n",
      "\n",
      "Epoch 144/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1816 Acc: 0.8000\n",
      "\n",
      "Epoch 145/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1843 Acc: 0.8000\n",
      "\n",
      "Epoch 146/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1866 Acc: 0.8000\n",
      "\n",
      "Epoch 147/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1875 Acc: 0.8000\n",
      "\n",
      "Epoch 148/200\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 1.1867 Acc: 0.8000\n",
      "\n",
      "Epoch 149/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.1865 Acc: 0.8000\n",
      "\n",
      "Epoch 150/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.1890 Acc: 0.8000\n",
      "\n",
      "Epoch 151/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.1930 Acc: 0.8000\n",
      "\n",
      "Epoch 152/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.1960 Acc: 0.8000\n",
      "\n",
      "Epoch 153/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2002 Acc: 0.8000\n",
      "\n",
      "Epoch 154/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2053 Acc: 0.8000\n",
      "\n",
      "Epoch 155/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2087 Acc: 0.8000\n",
      "\n",
      "Epoch 156/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2107 Acc: 0.8000\n",
      "\n",
      "Epoch 157/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2152 Acc: 0.8000\n",
      "\n",
      "Epoch 158/200\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 1.2200 Acc: 0.8000\n",
      "\n",
      "Epoch 159/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2240 Acc: 0.8000\n",
      "\n",
      "Epoch 160/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2253 Acc: 0.8000\n",
      "\n",
      "Epoch 161/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2273 Acc: 0.8000\n",
      "\n",
      "Epoch 162/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2305 Acc: 0.8000\n",
      "\n",
      "Epoch 163/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2327 Acc: 0.8000\n",
      "\n",
      "Epoch 164/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2323 Acc: 0.8000\n",
      "\n",
      "Epoch 165/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2330 Acc: 0.8000\n",
      "\n",
      "Epoch 166/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2336 Acc: 0.8000\n",
      "\n",
      "Epoch 167/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2348 Acc: 0.8000\n",
      "\n",
      "Epoch 168/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2348 Acc: 0.8000\n",
      "\n",
      "Epoch 169/200\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 1.2364 Acc: 0.8000\n",
      "\n",
      "Epoch 170/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2391 Acc: 0.8000\n",
      "\n",
      "Epoch 171/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2455 Acc: 0.8000\n",
      "\n",
      "Epoch 172/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2509 Acc: 0.8000\n",
      "\n",
      "Epoch 173/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2556 Acc: 0.8000\n",
      "\n",
      "Epoch 174/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2591 Acc: 0.8000\n",
      "\n",
      "Epoch 175/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2615 Acc: 0.8000\n",
      "\n",
      "Epoch 176/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2650 Acc: 0.8000\n",
      "\n",
      "Epoch 177/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2675 Acc: 0.8000\n",
      "\n",
      "Epoch 178/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2697 Acc: 0.8000\n",
      "\n",
      "Epoch 179/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2713 Acc: 0.8000\n",
      "\n",
      "Epoch 180/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2713 Acc: 0.8000\n",
      "\n",
      "Epoch 181/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2721 Acc: 0.8000\n",
      "\n",
      "Epoch 182/200\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 1.2742 Acc: 0.8000\n",
      "\n",
      "Epoch 183/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2774 Acc: 0.8000\n",
      "\n",
      "Epoch 184/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2788 Acc: 0.8000\n",
      "\n",
      "Epoch 185/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2805 Acc: 0.8000\n",
      "\n",
      "Epoch 186/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2807 Acc: 0.8000\n",
      "\n",
      "Epoch 187/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2809 Acc: 0.8000\n",
      "\n",
      "Epoch 188/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2817 Acc: 0.8000\n",
      "\n",
      "Epoch 189/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2847 Acc: 0.8000\n",
      "\n",
      "Epoch 190/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2881 Acc: 0.8000\n",
      "\n",
      "Epoch 191/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2901 Acc: 0.8000\n",
      "\n",
      "Epoch 192/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2916 Acc: 0.8000\n",
      "\n",
      "Epoch 193/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2946 Acc: 0.8000\n",
      "\n",
      "Epoch 194/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2973 Acc: 0.8000\n",
      "\n",
      "Epoch 195/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2990 Acc: 0.8000\n",
      "\n",
      "Epoch 196/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.3000 Acc: 0.8000\n",
      "\n",
      "Epoch 197/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.3001 Acc: 0.8000\n",
      "\n",
      "Epoch 198/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2999 Acc: 0.8000\n",
      "\n",
      "Epoch 199/200\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 1.0000\n",
      "val Loss: 1.2996 Acc: 0.8000\n",
      "\n",
      "Epoch 200/200\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 1.0000\n",
      "val Loss: 1.3009 Acc: 0.8000\n",
      "\n",
      "Training complete in 1m 6s\n",
      "Best val Acc: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# GRU with attention 모델 학습\n",
    "gru, gru_val_acc_history = train_model(gru, dataloaders_dict, criterion, num_epochs,\n",
    "                                       optimizer=optim.Adam(gru.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy0wgfcN_dCG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51kxme2L_dCG"
   },
   "source": [
    "# <br>__4. Testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:44.507718Z",
     "iopub.status.busy": "2022-01-10T12:16:44.507289Z",
     "iopub.status.idle": "2022-01-10T12:16:44.515428Z",
     "shell.execute_reply": "2022-01-10T12:16:44.514444Z",
     "shell.execute_reply.started": "2022-01-10T12:16:44.507672Z"
    },
    "id": "6IKsj0Z8_dCG"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "    \n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # accuracy를 도출함\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:16:44.517679Z",
     "iopub.status.busy": "2022-01-10T12:16:44.517097Z",
     "iopub.status.idle": "2022-01-10T12:16:44.546828Z",
     "shell.execute_reply": "2022-01-10T12:16:44.545761Z",
     "shell.execute_reply.started": "2022-01-10T12:16:44.517635Z"
    },
    "id": "IAxWs2M9_dCG",
    "outputId": "a2c7ba35-689b-4f95-85a4-9b0e9cecb063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# GRU with attention 모델 검증하기 (Acc: 0.8889)\n",
    "# Benchmark model인 GRU(Acc: 0.7556)와 비교했을 때, Attetion의 적용이 성능 향상에 도움이 됨을 알 수 있음\n",
    "test_model(gru, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlG2hJRo_dCG"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "03_RNN_Attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
