{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps5QLuNm8Ro9"
   },
   "source": [
    "# <br>[ LG전자_DX_Intensive_Course  ] 딥러닝 기반 시계열 분석 4<br><br> : Recurrent Neural Network - RNN, LSTM, GRU for Classification<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github에서 데이터 불러오기\n",
    "!git clone https://github.com/KU-DIC/LG_time_series_day11.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:25.366581Z",
     "iopub.status.busy": "2022-01-10T12:00:25.365967Z",
     "iopub.status.idle": "2022-01-10T12:00:26.825591Z",
     "shell.execute_reply": "2022-01-10T12:00:26.824862Z",
     "shell.execute_reply.started": "2022-01-10T12:00:25.366480Z"
    },
    "id": "KpuXcM8K8RpB"
   },
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCv_2B_a8RpD"
   },
   "source": [
    "# <br>0. Hyperparameter Setting\n",
    "- data_dir: 데이터가 존재하는 경로 (해당 실습에서는 train/test 시계열 데이터가 존재하는 경로를 의미함)\n",
    "- batch_size: 학습 및 검증에 사용할 배치의 크기\n",
    "- num_classes: 새로운 데이터의 class 개수\n",
    "- num_epochs: 학습할 epoch 횟수\n",
    "- window_size: input의 시간 길이 (time series data에서 도출한 subsequence의 길이)\n",
    "- input_size: 변수 개수\n",
    "- hidden_size: 모델의 hidden dimension\n",
    "- num_layers: 모델의 layer 개수\n",
    "- bidirectional: 모델의 양방향성 여부\n",
    "- random_seed: reproduction을 위해 고정할 seed의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:26.827471Z",
     "iopub.status.busy": "2022-01-10T12:00:26.827222Z",
     "iopub.status.idle": "2022-01-10T12:00:26.880843Z",
     "shell.execute_reply": "2022-01-10T12:00:26.880061Z",
     "shell.execute_reply.started": "2022-01-10T12:00:26.827437Z"
    },
    "id": "tiq3YmwX8RpE"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter setting\n",
    "data_dir = '/content/LG_time_series_day11/input/har-data'\n",
    "batch_size = 32\n",
    "num_classes = 6\n",
    "num_epochs = 200\n",
    "window_size = 50\n",
    "input_size = 561\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "random_seed = 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:26.883696Z",
     "iopub.status.busy": "2022-01-10T12:00:26.883276Z",
     "iopub.status.idle": "2022-01-10T12:00:26.891157Z",
     "shell.execute_reply": "2022-01-10T12:00:26.890520Z",
     "shell.execute_reply.started": "2022-01-10T12:00:26.883657Z"
    },
    "id": "mkBlPbF98RpE"
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCvkBVbI8RpF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqXreFc68RpG"
   },
   "source": [
    "# <br>__1. Data: Human Activity Recognition Data__\n",
    "- 데이터 description\n",
    "    - Human Activity Recognition (HAR) Data는 30명의 실험자들이 각자 스마트폰을 허리에 착용하고 6가지 활동 (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying)을 수행할 때 측정된 센서 데이터로 구성된 데이터셋이다. 해당 데이터셋은 총 561개의 변수로 이루어져 있으며, 전체 데이터 중 70%는 train 데이터이고 나머지 30%는 test 데이터이다. HAR Data를 활용한 시계열 분류 task는 다변량 시계열 데이터를 input으로 받아 이를 다음 6가지 활동 중 하나의 class로 분류하는 것을 목표로 한다: 0(Walking), 1(Walking Upstairs), 2(Walking Downstairs), 3(Sitting), 4(Standing), 5(Laying). <br><br>\n",
    "\n",
    "- 변수 설명\n",
    "    - 독립변수(X): 여러 실험자에 대하여 561개의 변수를 281 시점동안 수집한 시계열 데이터 -> shape: (#실험자, 561, 281)\n",
    "    - 종속변수(Y): 시계열 데이터의 label - 0(Walking) / 1(Walking Upstairs) / 2(Walking Downstairs) / 3(Sitting) / 4(Standing) / 5(Laying) <br><br>\n",
    "\n",
    "- 데이터 출처\n",
    "    - https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:26.893744Z",
     "iopub.status.busy": "2022-01-10T12:00:26.893363Z",
     "iopub.status.idle": "2022-01-10T12:00:26.907909Z",
     "shell.execute_reply": "2022-01-10T12:00:26.907208Z",
     "shell.execute_reply.started": "2022-01-10T12:00:26.893709Z"
    },
    "id": "p328cX568RpH"
   },
   "outputs": [],
   "source": [
    "def create_classification_dataset(window_size, data_dir, batch_size):\n",
    "    # data_dir에 있는 train/test 데이터 불러오기\n",
    "    x = pickle.load(open(os.path.join(data_dir, 'x_train.pkl'), 'rb'))\n",
    "    y = pickle.load(open(os.path.join(data_dir, 'state_train.pkl'), 'rb'))\n",
    "    x_test = pickle.load(open(os.path.join(data_dir, 'x_test.pkl'), 'rb'))\n",
    "    y_test = pickle.load(open(os.path.join(data_dir, 'state_test.pkl'), 'rb'))\n",
    "\n",
    "    # train data를 시간순으로 8:2의 비율로 train/validation set으로 분할\n",
    "    n_train = int(0.8 * len(x))\n",
    "    n_valid = len(x) - n_train\n",
    "    n_test = len(x_test)\n",
    "    x_train, y_train = x[:n_train], y[:n_train]\n",
    "    x_valid, y_valid = x[n_train:], y[n_train:]\n",
    "\n",
    "    # train/validation/test 데이터를 window_size 시점 길이로 분할\n",
    "    datasets = []\n",
    "    for set in [(x_train, y_train, n_train), (x_valid, y_valid, n_valid), (x_test, y_test, n_test)]:\n",
    "        T = set[0].shape[-1]\n",
    "        windows = np.split(set[0][:, :, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        windows = np.concatenate(windows, 0)\n",
    "        labels = np.split(set[1][:, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        labels = np.round(np.mean(np.concatenate(labels, 0), -1))\n",
    "        datasets.append(torch.utils.data.TensorDataset(torch.Tensor(windows), torch.Tensor(labels)))\n",
    "\n",
    "    # train/validation/test DataLoader 구축\n",
    "    trainset, validset, testset = datasets[0], datasets[1], datasets[2]\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:26.911208Z",
     "iopub.status.busy": "2022-01-10T12:00:26.910580Z",
     "iopub.status.idle": "2022-01-10T12:00:28.491064Z",
     "shell.execute_reply": "2022-01-10T12:00:28.490287Z",
     "shell.execute_reply.started": "2022-01-10T12:00:26.911162Z"
    },
    "id": "DLzC1M4R8RpI"
   },
   "outputs": [],
   "source": [
    "# Dataloader 구축\n",
    "# data shape: (batch_size x input_size x seq_len)\n",
    "train_loader, valid_loader, test_loader = create_classification_dataset(window_size, data_dir, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fag9K3gS8RpI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li04LKRi8RpJ"
   },
   "source": [
    "# <br>__2. Model: Vanilla RNN, LSTM, GRU__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSRB3MG78RpJ"
   },
   "source": [
    "- RNN/LSTM/GRU layer 설명 - **torch.nn.RNN() / torch.nn.LSTM() / torch.nn.GRU()**\n",
    "    - input_size: input feature의 크기 (시계열 데이터의 변수 개수)\n",
    "    - hidden_size: hidden state의 feature의 크기\n",
    "    - num_layers: recurrent layer의 개수\n",
    "    - batch_first: input의 shape에서 첫번째가 batch_size인지의 여부 (True - shape of (batch, seq, feature) / False - shape of (seq, batch, feature))\n",
    "    - bidirectional: 모델의 양방향성 여부 <br><br>\n",
    "    \n",
    "- 모델 ouputs\n",
    "    - output: last layer의 모든 시점의 hidden state\n",
    "        - output: tensor of shape (batch, seq, D * hidden_size) with batch_first=True / tensor of shape (seq, batch, D * hidden_size) with batch_first=False\n",
    "    - h_n: 모든 layer의 final hidden state\n",
    "        - h_n: tensor of shape (D * num_layers, batch, hidden_size)\n",
    "    - c_n: 모든 layer의 final cell state **(only LSTM)**\n",
    "        - c_n: tensor of shape (D * num_layers, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:28.492947Z",
     "iopub.status.busy": "2022-01-10T12:00:28.492676Z",
     "iopub.status.idle": "2022-01-10T12:00:28.512816Z",
     "shell.execute_reply": "2022-01-10T12:00:28.509920Z",
     "shell.execute_reply.started": "2022-01-10T12:00:28.492909Z"
    },
    "id": "pG2oNcq58RpK"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type='rnn'):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        \n",
    "        # rnn_type에 따른 recurrent layer 설정\n",
    "        if self.rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # bidirectional에 따른 fc layer 구축\n",
    "        # bidirectional 여부에 따라 hidden state의 shape가 달라짐 (True: 2 * hidden_size, False: hidden_size)\n",
    "        self.fc = nn.Linear(self.num_directions * hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # initial hidden states 설정\n",
    "        h0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # 선택한 rnn_type의 RNN으로부터 output 도출\n",
    "        if self.rnn_type in ['rnn', 'gru']:\n",
    "            out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        else:\n",
    "            # initial cell states 설정\n",
    "            c0 = torch.zeros(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            out, _ = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:28.516635Z",
     "iopub.status.busy": "2022-01-10T12:00:28.515308Z",
     "iopub.status.idle": "2022-01-10T12:00:32.231697Z",
     "shell.execute_reply": "2022-01-10T12:00:32.230026Z",
     "shell.execute_reply.started": "2022-01-10T12:00:28.516591Z"
    },
    "id": "7BHYnwXa8RpK",
    "outputId": "36f2a037-c2fd-44c9-d914-51c68edb8e9c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(561, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Vanilla RNN 모델 구축\n",
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type='rnn')\n",
    "rnn = rnn.to(device)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.233381Z",
     "iopub.status.busy": "2022-01-10T12:00:32.233110Z",
     "iopub.status.idle": "2022-01-10T12:00:32.247109Z",
     "shell.execute_reply": "2022-01-10T12:00:32.246125Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.233346Z"
    },
    "id": "YNthwKIq8RpL",
    "outputId": "50bbd9f6-19d2-459b-d11e-5f6f503b52c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(561, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 구축\n",
    "lstm = RNN(input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type='lstm')\n",
    "lstm = lstm.to(device)\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.248855Z",
     "iopub.status.busy": "2022-01-10T12:00:32.248585Z",
     "iopub.status.idle": "2022-01-10T12:00:32.261483Z",
     "shell.execute_reply": "2022-01-10T12:00:32.260803Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.248820Z"
    },
    "id": "bPHGc4d28RpL",
    "outputId": "e4e2070a-c1a2-4c01-ea1c-2fa4ff40108b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): GRU(561, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델 구축\n",
    "gru = RNN(input_size, hidden_size, num_layers, num_classes, bidirectional, rnn_type='gru')\n",
    "gru = gru.to(device)\n",
    "print(gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxPQQiXA8RpM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvqtFh5q8RpM"
   },
   "source": [
    "# <br>__3. Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.264570Z",
     "iopub.status.busy": "2022-01-10T12:00:32.264335Z",
     "iopub.status.idle": "2022-01-10T12:00:32.278744Z",
     "shell.execute_reply": "2022-01-10T12:00:32.277864Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.264538Z"
    },
    "id": "3ps6oy-s8RpN"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += labels.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "            epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.280497Z",
     "iopub.status.busy": "2022-01-10T12:00:32.280182Z",
     "iopub.status.idle": "2022-01-10T12:00:32.290262Z",
     "shell.execute_reply": "2022-01-10T12:00:32.289577Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.280462Z"
    },
    "id": "PW8gswxF8RpO"
   },
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_loader,\n",
    "    'val': valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.292115Z",
     "iopub.status.busy": "2022-01-10T12:00:32.291868Z",
     "iopub.status.idle": "2022-01-10T12:00:32.298579Z",
     "shell.execute_reply": "2022-01-10T12:00:32.297924Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.292084Z"
    },
    "id": "XiBRjXyB8RpO"
   },
   "outputs": [],
   "source": [
    "# loss function 설정\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:32.300338Z",
     "iopub.status.busy": "2022-01-10T12:00:32.300020Z",
     "iopub.status.idle": "2022-01-10T12:00:39.719378Z",
     "shell.execute_reply": "2022-01-10T12:00:39.718640Z",
     "shell.execute_reply.started": "2022-01-10T12:00:32.300305Z"
    },
    "id": "CmC3Zlaj8RpO",
    "outputId": "db09c79a-5aea-4e81-b9df-917561e6e8ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n",
      "train Loss: 1.7269 Acc: 0.1625\n",
      "val Loss: 1.5952 Acc: 0.3600\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train Loss: 1.5923 Acc: 0.3625\n",
      "val Loss: 1.4803 Acc: 0.5200\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train Loss: 1.5066 Acc: 0.4250\n",
      "val Loss: 1.3984 Acc: 0.5200\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train Loss: 1.4346 Acc: 0.4250\n",
      "val Loss: 1.3447 Acc: 0.5200\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train Loss: 1.3826 Acc: 0.4250\n",
      "val Loss: 1.3092 Acc: 0.5200\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train Loss: 1.3376 Acc: 0.4250\n",
      "val Loss: 1.2846 Acc: 0.5200\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train Loss: 1.3015 Acc: 0.4375\n",
      "val Loss: 1.2660 Acc: 0.5200\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train Loss: 1.2709 Acc: 0.4875\n",
      "val Loss: 1.2510 Acc: 0.5200\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train Loss: 1.2469 Acc: 0.5000\n",
      "val Loss: 1.2446 Acc: 0.5200\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train Loss: 1.2242 Acc: 0.5375\n",
      "val Loss: 1.2393 Acc: 0.5600\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train Loss: 1.2062 Acc: 0.5375\n",
      "val Loss: 1.2287 Acc: 0.5600\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train Loss: 1.1907 Acc: 0.5500\n",
      "val Loss: 1.2197 Acc: 0.5600\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train Loss: 1.1732 Acc: 0.5500\n",
      "val Loss: 1.2025 Acc: 0.6400\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train Loss: 1.1570 Acc: 0.5625\n",
      "val Loss: 1.1861 Acc: 0.6400\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train Loss: 1.1424 Acc: 0.5750\n",
      "val Loss: 1.1663 Acc: 0.6800\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train Loss: 1.1280 Acc: 0.5875\n",
      "val Loss: 1.1514 Acc: 0.6800\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train Loss: 1.1166 Acc: 0.5750\n",
      "val Loss: 1.1371 Acc: 0.6400\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train Loss: 1.1043 Acc: 0.6125\n",
      "val Loss: 1.1298 Acc: 0.6400\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train Loss: 1.0924 Acc: 0.6125\n",
      "val Loss: 1.1188 Acc: 0.6400\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train Loss: 1.0827 Acc: 0.5875\n",
      "val Loss: 1.1105 Acc: 0.6400\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train Loss: 1.0737 Acc: 0.5875\n",
      "val Loss: 1.1022 Acc: 0.6400\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train Loss: 1.0637 Acc: 0.5625\n",
      "val Loss: 1.1021 Acc: 0.6400\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train Loss: 1.0534 Acc: 0.5875\n",
      "val Loss: 1.1040 Acc: 0.6400\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train Loss: 1.0440 Acc: 0.6125\n",
      "val Loss: 1.1044 Acc: 0.6400\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train Loss: 1.0360 Acc: 0.6375\n",
      "val Loss: 1.1004 Acc: 0.6400\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train Loss: 1.0277 Acc: 0.6500\n",
      "val Loss: 1.0955 Acc: 0.6400\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train Loss: 1.0192 Acc: 0.6500\n",
      "val Loss: 1.0829 Acc: 0.6400\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train Loss: 1.0127 Acc: 0.6625\n",
      "val Loss: 1.0708 Acc: 0.6400\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train Loss: 1.0036 Acc: 0.6375\n",
      "val Loss: 1.0684 Acc: 0.6400\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train Loss: 0.9954 Acc: 0.6250\n",
      "val Loss: 1.0691 Acc: 0.6400\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train Loss: 0.9866 Acc: 0.6625\n",
      "val Loss: 1.0744 Acc: 0.6400\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train Loss: 0.9809 Acc: 0.6750\n",
      "val Loss: 1.0777 Acc: 0.6400\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train Loss: 0.9727 Acc: 0.6750\n",
      "val Loss: 1.0746 Acc: 0.6400\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train Loss: 0.9637 Acc: 0.6875\n",
      "val Loss: 1.0593 Acc: 0.6400\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.6875\n",
      "val Loss: 1.0505 Acc: 0.6400\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train Loss: 0.9473 Acc: 0.6875\n",
      "val Loss: 1.0426 Acc: 0.6400\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train Loss: 0.9414 Acc: 0.6875\n",
      "val Loss: 1.0392 Acc: 0.6400\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train Loss: 0.9333 Acc: 0.7000\n",
      "val Loss: 1.0426 Acc: 0.6400\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train Loss: 0.9255 Acc: 0.7000\n",
      "val Loss: 1.0407 Acc: 0.6400\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train Loss: 0.9179 Acc: 0.7000\n",
      "val Loss: 1.0434 Acc: 0.6400\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train Loss: 0.9091 Acc: 0.7000\n",
      "val Loss: 1.0372 Acc: 0.6400\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train Loss: 0.9027 Acc: 0.7125\n",
      "val Loss: 1.0380 Acc: 0.6400\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train Loss: 0.8920 Acc: 0.7125\n",
      "val Loss: 1.0265 Acc: 0.6400\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train Loss: 0.8874 Acc: 0.7000\n",
      "val Loss: 1.0119 Acc: 0.6400\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train Loss: 0.8819 Acc: 0.7000\n",
      "val Loss: 1.0119 Acc: 0.6400\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train Loss: 0.8710 Acc: 0.7125\n",
      "val Loss: 1.0095 Acc: 0.6400\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train Loss: 0.8632 Acc: 0.7125\n",
      "val Loss: 1.0082 Acc: 0.6400\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train Loss: 0.8515 Acc: 0.7250\n",
      "val Loss: 1.0137 Acc: 0.6400\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train Loss: 0.8452 Acc: 0.7250\n",
      "val Loss: 1.0186 Acc: 0.6400\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train Loss: 0.8383 Acc: 0.7250\n",
      "val Loss: 1.0145 Acc: 0.6400\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train Loss: 0.8288 Acc: 0.7375\n",
      "val Loss: 0.9953 Acc: 0.6400\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train Loss: 0.8198 Acc: 0.7375\n",
      "val Loss: 0.9839 Acc: 0.6400\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train Loss: 0.8114 Acc: 0.7375\n",
      "val Loss: 0.9810 Acc: 0.6400\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train Loss: 0.8032 Acc: 0.7375\n",
      "val Loss: 0.9763 Acc: 0.6400\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train Loss: 0.7950 Acc: 0.7500\n",
      "val Loss: 0.9831 Acc: 0.6400\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train Loss: 0.7875 Acc: 0.7500\n",
      "val Loss: 0.9780 Acc: 0.6400\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train Loss: 0.7759 Acc: 0.7500\n",
      "val Loss: 0.9638 Acc: 0.6400\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train Loss: 0.7707 Acc: 0.7500\n",
      "val Loss: 0.9512 Acc: 0.6400\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train Loss: 0.7639 Acc: 0.7250\n",
      "val Loss: 0.9508 Acc: 0.6800\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train Loss: 0.7568 Acc: 0.7375\n",
      "val Loss: 0.9655 Acc: 0.6400\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train Loss: 0.7444 Acc: 0.7625\n",
      "val Loss: 0.9623 Acc: 0.6800\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train Loss: 0.7350 Acc: 0.7625\n",
      "val Loss: 0.9556 Acc: 0.6400\n",
      "\n",
      "Epoch 63/200\n",
      "----------\n",
      "train Loss: 0.7274 Acc: 0.7625\n",
      "val Loss: 0.9445 Acc: 0.6800\n",
      "\n",
      "Epoch 64/200\n",
      "----------\n",
      "train Loss: 0.7170 Acc: 0.7750\n",
      "val Loss: 0.9427 Acc: 0.6800\n",
      "\n",
      "Epoch 65/200\n",
      "----------\n",
      "train Loss: 0.7106 Acc: 0.7750\n",
      "val Loss: 0.9429 Acc: 0.7200\n",
      "\n",
      "Epoch 66/200\n",
      "----------\n",
      "train Loss: 0.7006 Acc: 0.7750\n",
      "val Loss: 0.9304 Acc: 0.6800\n",
      "\n",
      "Epoch 67/200\n",
      "----------\n",
      "train Loss: 0.6939 Acc: 0.7750\n",
      "val Loss: 0.9231 Acc: 0.6800\n",
      "\n",
      "Epoch 68/200\n",
      "----------\n",
      "train Loss: 0.6883 Acc: 0.7625\n",
      "val Loss: 0.9108 Acc: 0.6800\n",
      "\n",
      "Epoch 69/200\n",
      "----------\n",
      "train Loss: 0.6829 Acc: 0.7750\n",
      "val Loss: 0.9206 Acc: 0.6800\n",
      "\n",
      "Epoch 70/200\n",
      "----------\n",
      "train Loss: 0.6664 Acc: 0.7750\n",
      "val Loss: 0.9142 Acc: 0.6800\n",
      "\n",
      "Epoch 71/200\n",
      "----------\n",
      "train Loss: 0.6586 Acc: 0.7750\n",
      "val Loss: 0.9069 Acc: 0.6800\n",
      "\n",
      "Epoch 72/200\n",
      "----------\n",
      "train Loss: 0.6512 Acc: 0.7875\n",
      "val Loss: 0.8986 Acc: 0.6800\n",
      "\n",
      "Epoch 73/200\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.7750\n",
      "val Loss: 0.8839 Acc: 0.6800\n",
      "\n",
      "Epoch 74/200\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.7625\n",
      "val Loss: 0.8891 Acc: 0.6800\n",
      "\n",
      "Epoch 75/200\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.7875\n",
      "val Loss: 0.8902 Acc: 0.7200\n",
      "\n",
      "Epoch 76/200\n",
      "----------\n",
      "train Loss: 0.6204 Acc: 0.8125\n",
      "val Loss: 0.8936 Acc: 0.7200\n",
      "\n",
      "Epoch 77/200\n",
      "----------\n",
      "train Loss: 0.6092 Acc: 0.8125\n",
      "val Loss: 0.8786 Acc: 0.6800\n",
      "\n",
      "Epoch 78/200\n",
      "----------\n",
      "train Loss: 0.5999 Acc: 0.8125\n",
      "val Loss: 0.8681 Acc: 0.6800\n",
      "\n",
      "Epoch 79/200\n",
      "----------\n",
      "train Loss: 0.6012 Acc: 0.7875\n",
      "val Loss: 0.8659 Acc: 0.6800\n",
      "\n",
      "Epoch 80/200\n",
      "----------\n",
      "train Loss: 0.5901 Acc: 0.8000\n",
      "val Loss: 0.8765 Acc: 0.7600\n",
      "\n",
      "Epoch 81/200\n",
      "----------\n",
      "train Loss: 0.5776 Acc: 0.8375\n",
      "val Loss: 0.8754 Acc: 0.7600\n",
      "\n",
      "Epoch 82/200\n",
      "----------\n",
      "train Loss: 0.5711 Acc: 0.8375\n",
      "val Loss: 0.8634 Acc: 0.7600\n",
      "\n",
      "Epoch 83/200\n",
      "----------\n",
      "train Loss: 0.5643 Acc: 0.8375\n",
      "val Loss: 0.8593 Acc: 0.7600\n",
      "\n",
      "Epoch 84/200\n",
      "----------\n",
      "train Loss: 0.5539 Acc: 0.8375\n",
      "val Loss: 0.8730 Acc: 0.7200\n",
      "\n",
      "Epoch 85/200\n",
      "----------\n",
      "train Loss: 0.5489 Acc: 0.8500\n",
      "val Loss: 0.8588 Acc: 0.7600\n",
      "\n",
      "Epoch 86/200\n",
      "----------\n",
      "train Loss: 0.5399 Acc: 0.8375\n",
      "val Loss: 0.8421 Acc: 0.7200\n",
      "\n",
      "Epoch 87/200\n",
      "----------\n",
      "train Loss: 0.5317 Acc: 0.8375\n",
      "val Loss: 0.8390 Acc: 0.7200\n",
      "\n",
      "Epoch 88/200\n",
      "----------\n",
      "train Loss: 0.5214 Acc: 0.8500\n",
      "val Loss: 0.8432 Acc: 0.7600\n",
      "\n",
      "Epoch 89/200\n",
      "----------\n",
      "train Loss: 0.5144 Acc: 0.8500\n",
      "val Loss: 0.8459 Acc: 0.7600\n",
      "\n",
      "Epoch 90/200\n",
      "----------\n",
      "train Loss: 0.5071 Acc: 0.8500\n",
      "val Loss: 0.8416 Acc: 0.7200\n",
      "\n",
      "Epoch 91/200\n",
      "----------\n",
      "train Loss: 0.5010 Acc: 0.8500\n",
      "val Loss: 0.8410 Acc: 0.7200\n",
      "\n",
      "Epoch 92/200\n",
      "----------\n",
      "train Loss: 0.4912 Acc: 0.8500\n",
      "val Loss: 0.8472 Acc: 0.7600\n",
      "\n",
      "Epoch 93/200\n",
      "----------\n",
      "train Loss: 0.4838 Acc: 0.8625\n",
      "val Loss: 0.8547 Acc: 0.7200\n",
      "\n",
      "Epoch 94/200\n",
      "----------\n",
      "train Loss: 0.4777 Acc: 0.8625\n",
      "val Loss: 0.8506 Acc: 0.7200\n",
      "\n",
      "Epoch 95/200\n",
      "----------\n",
      "train Loss: 0.4688 Acc: 0.8625\n",
      "val Loss: 0.8574 Acc: 0.7200\n",
      "\n",
      "Epoch 96/200\n",
      "----------\n",
      "train Loss: 0.4638 Acc: 0.8625\n",
      "val Loss: 0.8620 Acc: 0.7200\n",
      "\n",
      "Epoch 97/200\n",
      "----------\n",
      "train Loss: 0.4568 Acc: 0.8750\n",
      "val Loss: 0.8852 Acc: 0.7200\n",
      "\n",
      "Epoch 98/200\n",
      "----------\n",
      "train Loss: 0.4540 Acc: 0.8625\n",
      "val Loss: 0.8704 Acc: 0.7600\n",
      "\n",
      "Epoch 99/200\n",
      "----------\n",
      "train Loss: 0.4358 Acc: 0.9000\n",
      "val Loss: 0.8690 Acc: 0.7200\n",
      "\n",
      "Epoch 100/200\n",
      "----------\n",
      "train Loss: 0.4497 Acc: 0.8625\n",
      "val Loss: 0.8641 Acc: 0.7600\n",
      "\n",
      "Epoch 101/200\n",
      "----------\n",
      "train Loss: 0.4529 Acc: 0.8500\n",
      "val Loss: 0.9507 Acc: 0.6000\n",
      "\n",
      "Epoch 102/200\n",
      "----------\n",
      "train Loss: 0.4577 Acc: 0.8500\n",
      "val Loss: 0.8576 Acc: 0.6800\n",
      "\n",
      "Epoch 103/200\n",
      "----------\n",
      "train Loss: 0.4221 Acc: 0.8625\n",
      "val Loss: 0.8683 Acc: 0.7200\n",
      "\n",
      "Epoch 104/200\n",
      "----------\n",
      "train Loss: 0.4161 Acc: 0.8750\n",
      "val Loss: 0.8796 Acc: 0.7200\n",
      "\n",
      "Epoch 105/200\n",
      "----------\n",
      "train Loss: 0.4107 Acc: 0.8875\n",
      "val Loss: 0.8994 Acc: 0.7200\n",
      "\n",
      "Epoch 106/200\n",
      "----------\n",
      "train Loss: 0.3963 Acc: 0.9000\n",
      "val Loss: 0.8746 Acc: 0.6800\n",
      "\n",
      "Epoch 107/200\n",
      "----------\n",
      "train Loss: 0.4128 Acc: 0.8625\n",
      "val Loss: 0.8682 Acc: 0.6800\n",
      "\n",
      "Epoch 108/200\n",
      "----------\n",
      "train Loss: 0.3817 Acc: 0.8875\n",
      "val Loss: 0.8869 Acc: 0.7200\n",
      "\n",
      "Epoch 109/200\n",
      "----------\n",
      "train Loss: 0.3836 Acc: 0.9000\n",
      "val Loss: 0.8785 Acc: 0.7200\n",
      "\n",
      "Epoch 110/200\n",
      "----------\n",
      "train Loss: 0.3672 Acc: 0.9125\n",
      "val Loss: 0.8808 Acc: 0.7200\n",
      "\n",
      "Epoch 111/200\n",
      "----------\n",
      "train Loss: 0.3690 Acc: 0.9000\n",
      "val Loss: 0.8813 Acc: 0.7200\n",
      "\n",
      "Epoch 112/200\n",
      "----------\n",
      "train Loss: 0.3591 Acc: 0.9125\n",
      "val Loss: 0.8768 Acc: 0.7200\n",
      "\n",
      "Epoch 113/200\n",
      "----------\n",
      "train Loss: 0.3579 Acc: 0.8875\n",
      "val Loss: 0.8779 Acc: 0.6800\n",
      "\n",
      "Epoch 114/200\n",
      "----------\n",
      "train Loss: 0.3438 Acc: 0.9250\n",
      "val Loss: 0.8939 Acc: 0.7200\n",
      "\n",
      "Epoch 115/200\n",
      "----------\n",
      "train Loss: 0.3756 Acc: 0.8625\n",
      "val Loss: 0.8702 Acc: 0.7200\n",
      "\n",
      "Epoch 116/200\n",
      "----------\n",
      "train Loss: 0.3564 Acc: 0.9250\n",
      "val Loss: 1.0023 Acc: 0.6000\n",
      "\n",
      "Epoch 117/200\n",
      "----------\n",
      "train Loss: 0.4003 Acc: 0.8625\n",
      "val Loss: 0.9047 Acc: 0.6800\n",
      "\n",
      "Epoch 118/200\n",
      "----------\n",
      "train Loss: 0.3464 Acc: 0.9125\n",
      "val Loss: 0.8896 Acc: 0.7200\n",
      "\n",
      "Epoch 119/200\n",
      "----------\n",
      "train Loss: 0.3488 Acc: 0.8750\n",
      "val Loss: 0.9253 Acc: 0.6800\n",
      "\n",
      "Epoch 120/200\n",
      "----------\n",
      "train Loss: 0.3363 Acc: 0.9250\n",
      "val Loss: 0.9135 Acc: 0.6800\n",
      "\n",
      "Epoch 121/200\n",
      "----------\n",
      "train Loss: 0.3204 Acc: 0.9250\n",
      "val Loss: 0.8936 Acc: 0.6800\n",
      "\n",
      "Epoch 122/200\n",
      "----------\n",
      "train Loss: 0.3223 Acc: 0.9125\n",
      "val Loss: 0.8862 Acc: 0.7200\n",
      "\n",
      "Epoch 123/200\n",
      "----------\n",
      "train Loss: 0.3045 Acc: 0.9250\n",
      "val Loss: 0.8894 Acc: 0.7200\n",
      "\n",
      "Epoch 124/200\n",
      "----------\n",
      "train Loss: 0.3051 Acc: 0.9250\n",
      "val Loss: 0.8848 Acc: 0.7200\n",
      "\n",
      "Epoch 125/200\n",
      "----------\n",
      "train Loss: 0.2945 Acc: 0.9500\n",
      "val Loss: 0.8891 Acc: 0.7200\n",
      "\n",
      "Epoch 126/200\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.9500\n",
      "val Loss: 0.9016 Acc: 0.7200\n",
      "\n",
      "Epoch 127/200\n",
      "----------\n",
      "train Loss: 0.2970 Acc: 0.9375\n",
      "val Loss: 0.8851 Acc: 0.7200\n",
      "\n",
      "Epoch 128/200\n",
      "----------\n",
      "train Loss: 0.2832 Acc: 0.9500\n",
      "val Loss: 0.8905 Acc: 0.7200\n",
      "\n",
      "Epoch 129/200\n",
      "----------\n",
      "train Loss: 0.2859 Acc: 0.9500\n",
      "val Loss: 0.9089 Acc: 0.7200\n",
      "\n",
      "Epoch 130/200\n",
      "----------\n",
      "train Loss: 0.2698 Acc: 0.9500\n",
      "val Loss: 0.9172 Acc: 0.7200\n",
      "\n",
      "Epoch 131/200\n",
      "----------\n",
      "train Loss: 0.2646 Acc: 0.9500\n",
      "val Loss: 0.9186 Acc: 0.7200\n",
      "\n",
      "Epoch 132/200\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.9500\n",
      "val Loss: 0.9286 Acc: 0.7200\n",
      "\n",
      "Epoch 133/200\n",
      "----------\n",
      "train Loss: 0.2572 Acc: 0.9500\n",
      "val Loss: 0.9190 Acc: 0.7200\n",
      "\n",
      "Epoch 134/200\n",
      "----------\n",
      "train Loss: 0.2539 Acc: 0.9625\n",
      "val Loss: 0.9222 Acc: 0.7200\n",
      "\n",
      "Epoch 135/200\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.9000\n",
      "val Loss: 0.8928 Acc: 0.7200\n",
      "\n",
      "Epoch 136/200\n",
      "----------\n",
      "train Loss: 0.2646 Acc: 0.9500\n",
      "val Loss: 0.8962 Acc: 0.7200\n",
      "\n",
      "Epoch 137/200\n",
      "----------\n",
      "train Loss: 0.2434 Acc: 0.9750\n",
      "val Loss: 0.9335 Acc: 0.6800\n",
      "\n",
      "Epoch 138/200\n",
      "----------\n",
      "train Loss: 0.2534 Acc: 0.9375\n",
      "val Loss: 0.9015 Acc: 0.7200\n",
      "\n",
      "Epoch 139/200\n",
      "----------\n",
      "train Loss: 0.2636 Acc: 0.9500\n",
      "val Loss: 1.0430 Acc: 0.6400\n",
      "\n",
      "Epoch 140/200\n",
      "----------\n",
      "train Loss: 0.2740 Acc: 0.9375\n",
      "val Loss: 0.9219 Acc: 0.7200\n",
      "\n",
      "Epoch 141/200\n",
      "----------\n",
      "train Loss: 0.2542 Acc: 0.9125\n",
      "val Loss: 0.9221 Acc: 0.7200\n",
      "\n",
      "Epoch 142/200\n",
      "----------\n",
      "train Loss: 0.2508 Acc: 0.9500\n",
      "val Loss: 0.9777 Acc: 0.6800\n",
      "\n",
      "Epoch 143/200\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.9500\n",
      "val Loss: 0.9027 Acc: 0.7200\n",
      "\n",
      "Epoch 144/200\n",
      "----------\n",
      "train Loss: 0.2258 Acc: 0.9500\n",
      "val Loss: 0.9322 Acc: 0.6800\n",
      "\n",
      "Epoch 145/200\n",
      "----------\n",
      "train Loss: 0.2406 Acc: 0.9250\n",
      "val Loss: 0.9115 Acc: 0.7200\n",
      "\n",
      "Epoch 146/200\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9750\n",
      "val Loss: 0.9349 Acc: 0.6800\n",
      "\n",
      "Epoch 147/200\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.9750\n",
      "val Loss: 0.9237 Acc: 0.7200\n",
      "\n",
      "Epoch 148/200\n",
      "----------\n",
      "train Loss: 0.2120 Acc: 0.9500\n",
      "val Loss: 0.9383 Acc: 0.7200\n",
      "\n",
      "Epoch 149/200\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9500\n",
      "val Loss: 0.9369 Acc: 0.6800\n",
      "\n",
      "Epoch 150/200\n",
      "----------\n",
      "train Loss: 0.2050 Acc: 0.9625\n",
      "val Loss: 0.9332 Acc: 0.7200\n",
      "\n",
      "Epoch 151/200\n",
      "----------\n",
      "train Loss: 0.1979 Acc: 0.9750\n",
      "val Loss: 0.9467 Acc: 0.7200\n",
      "\n",
      "Epoch 152/200\n",
      "----------\n",
      "train Loss: 0.1922 Acc: 0.9750\n",
      "val Loss: 0.9346 Acc: 0.7200\n",
      "\n",
      "Epoch 153/200\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9875\n",
      "val Loss: 0.9390 Acc: 0.7200\n",
      "\n",
      "Epoch 154/200\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9875\n",
      "val Loss: 0.9630 Acc: 0.7200\n",
      "\n",
      "Epoch 155/200\n",
      "----------\n",
      "train Loss: 0.1801 Acc: 0.9750\n",
      "val Loss: 0.9727 Acc: 0.7200\n",
      "\n",
      "Epoch 156/200\n",
      "----------\n",
      "train Loss: 0.1808 Acc: 0.9625\n",
      "val Loss: 0.9822 Acc: 0.7200\n",
      "\n",
      "Epoch 157/200\n",
      "----------\n",
      "train Loss: 0.1773 Acc: 0.9750\n",
      "val Loss: 0.9735 Acc: 0.7200\n",
      "\n",
      "Epoch 158/200\n",
      "----------\n",
      "train Loss: 0.1696 Acc: 0.9875\n",
      "val Loss: 0.9689 Acc: 0.7200\n",
      "\n",
      "Epoch 159/200\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9875\n",
      "val Loss: 0.9699 Acc: 0.7200\n",
      "\n",
      "Epoch 160/200\n",
      "----------\n",
      "train Loss: 0.1651 Acc: 0.9875\n",
      "val Loss: 0.9760 Acc: 0.7200\n",
      "\n",
      "Epoch 161/200\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9875\n",
      "val Loss: 0.9887 Acc: 0.7200\n",
      "\n",
      "Epoch 162/200\n",
      "----------\n",
      "train Loss: 0.1639 Acc: 0.9750\n",
      "val Loss: 0.9862 Acc: 0.7200\n",
      "\n",
      "Epoch 163/200\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9875\n",
      "val Loss: 0.9850 Acc: 0.7200\n",
      "\n",
      "Epoch 164/200\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9750\n",
      "val Loss: 0.9861 Acc: 0.7200\n",
      "\n",
      "Epoch 165/200\n",
      "----------\n",
      "train Loss: 0.1524 Acc: 0.9750\n",
      "val Loss: 0.9969 Acc: 0.7200\n",
      "\n",
      "Epoch 166/200\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.9875\n",
      "val Loss: 1.0157 Acc: 0.7200\n",
      "\n",
      "Epoch 167/200\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9875\n",
      "val Loss: 1.0085 Acc: 0.6800\n",
      "\n",
      "Epoch 168/200\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.9750\n",
      "val Loss: 0.9861 Acc: 0.7200\n",
      "\n",
      "Epoch 169/200\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9875\n",
      "val Loss: 0.9976 Acc: 0.7200\n",
      "\n",
      "Epoch 170/200\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9875\n",
      "val Loss: 1.0083 Acc: 0.7200\n",
      "\n",
      "Epoch 171/200\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9750\n",
      "val Loss: 1.0199 Acc: 0.7200\n",
      "\n",
      "Epoch 172/200\n",
      "----------\n",
      "train Loss: 0.1404 Acc: 0.9875\n",
      "val Loss: 0.9995 Acc: 0.7200\n",
      "\n",
      "Epoch 173/200\n",
      "----------\n",
      "train Loss: 0.1276 Acc: 1.0000\n",
      "val Loss: 1.0277 Acc: 0.7200\n",
      "\n",
      "Epoch 174/200\n",
      "----------\n",
      "train Loss: 0.1495 Acc: 1.0000\n",
      "val Loss: 0.9957 Acc: 0.7200\n",
      "\n",
      "Epoch 175/200\n",
      "----------\n",
      "train Loss: 0.1415 Acc: 0.9875\n",
      "val Loss: 1.0021 Acc: 0.7200\n",
      "\n",
      "Epoch 176/200\n",
      "----------\n",
      "train Loss: 0.1299 Acc: 0.9875\n",
      "val Loss: 1.0097 Acc: 0.7200\n",
      "\n",
      "Epoch 177/200\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9875\n",
      "val Loss: 1.0105 Acc: 0.7200\n",
      "\n",
      "Epoch 178/200\n",
      "----------\n",
      "train Loss: 0.1193 Acc: 1.0000\n",
      "val Loss: 1.0137 Acc: 0.7200\n",
      "\n",
      "Epoch 179/200\n",
      "----------\n",
      "train Loss: 0.1138 Acc: 1.0000\n",
      "val Loss: 1.0097 Acc: 0.7200\n",
      "\n",
      "Epoch 180/200\n",
      "----------\n",
      "train Loss: 0.1166 Acc: 1.0000\n",
      "val Loss: 1.0104 Acc: 0.7200\n",
      "\n",
      "Epoch 181/200\n",
      "----------\n",
      "train Loss: 0.1088 Acc: 1.0000\n",
      "val Loss: 1.0202 Acc: 0.7200\n",
      "\n",
      "Epoch 182/200\n",
      "----------\n",
      "train Loss: 0.1097 Acc: 1.0000\n",
      "val Loss: 1.0071 Acc: 0.7200\n",
      "\n",
      "Epoch 183/200\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 1.0000\n",
      "val Loss: 1.0058 Acc: 0.7200\n",
      "\n",
      "Epoch 184/200\n",
      "----------\n",
      "train Loss: 0.1053 Acc: 1.0000\n",
      "val Loss: 1.0203 Acc: 0.7200\n",
      "\n",
      "Epoch 185/200\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 1.0000\n",
      "val Loss: 1.0282 Acc: 0.7200\n",
      "\n",
      "Epoch 186/200\n",
      "----------\n",
      "train Loss: 0.0969 Acc: 1.0000\n",
      "val Loss: 1.0376 Acc: 0.7200\n",
      "\n",
      "Epoch 187/200\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 1.0000\n",
      "val Loss: 1.0531 Acc: 0.7200\n",
      "\n",
      "Epoch 188/200\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 1.0000\n",
      "val Loss: 1.0458 Acc: 0.7200\n",
      "\n",
      "Epoch 189/200\n",
      "----------\n",
      "train Loss: 0.0922 Acc: 1.0000\n",
      "val Loss: 1.0438 Acc: 0.7200\n",
      "\n",
      "Epoch 190/200\n",
      "----------\n",
      "train Loss: 0.0900 Acc: 1.0000\n",
      "val Loss: 1.0460 Acc: 0.7200\n",
      "\n",
      "Epoch 191/200\n",
      "----------\n",
      "train Loss: 0.0901 Acc: 1.0000\n",
      "val Loss: 1.0434 Acc: 0.7200\n",
      "\n",
      "Epoch 192/200\n",
      "----------\n",
      "train Loss: 0.0872 Acc: 1.0000\n",
      "val Loss: 1.0626 Acc: 0.7200\n",
      "\n",
      "Epoch 193/200\n",
      "----------\n",
      "train Loss: 0.0915 Acc: 1.0000\n",
      "val Loss: 1.0656 Acc: 0.7200\n",
      "\n",
      "Epoch 194/200\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 1.0000\n",
      "val Loss: 1.0452 Acc: 0.7600\n",
      "\n",
      "Epoch 195/200\n",
      "----------\n",
      "train Loss: 0.0847 Acc: 1.0000\n",
      "val Loss: 1.0518 Acc: 0.7600\n",
      "\n",
      "Epoch 196/200\n",
      "----------\n",
      "train Loss: 0.0817 Acc: 1.0000\n",
      "val Loss: 1.0496 Acc: 0.7200\n",
      "\n",
      "Epoch 197/200\n",
      "----------\n",
      "train Loss: 0.0775 Acc: 1.0000\n",
      "val Loss: 1.0650 Acc: 0.7200\n",
      "\n",
      "Epoch 198/200\n",
      "----------\n",
      "train Loss: 0.0770 Acc: 1.0000\n",
      "val Loss: 1.0695 Acc: 0.7200\n",
      "\n",
      "Epoch 199/200\n",
      "----------\n",
      "train Loss: 0.0741 Acc: 1.0000\n",
      "val Loss: 1.0572 Acc: 0.7600\n",
      "\n",
      "Epoch 200/200\n",
      "----------\n",
      "train Loss: 0.0722 Acc: 1.0000\n",
      "val Loss: 1.0577 Acc: 0.7600\n",
      "\n",
      "Training complete in 0m 30s\n",
      "Best val Acc: 0.760000\n"
     ]
    }
   ],
   "source": [
    "# Vanilla RNN 모델 학습\n",
    "rnn, rnn_val_acc_history = train_model(rnn, dataloaders_dict, criterion, num_epochs,\n",
    "                                       optimizer=optim.Adam(rnn.parameters(), lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:39.720963Z",
     "iopub.status.busy": "2022-01-10T12:00:39.720540Z",
     "iopub.status.idle": "2022-01-10T12:00:46.413572Z",
     "shell.execute_reply": "2022-01-10T12:00:46.412817Z",
     "shell.execute_reply.started": "2022-01-10T12:00:39.720922Z"
    },
    "id": "fF-dd0np8RpP",
    "outputId": "753c396b-5373-4766-ce1b-863d233a02dc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n",
      "train Loss: 1.8070 Acc: 0.0500\n",
      "val Loss: 1.7599 Acc: 0.5600\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train Loss: 1.7755 Acc: 0.4000\n",
      "val Loss: 1.7251 Acc: 0.6400\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train Loss: 1.7458 Acc: 0.4750\n",
      "val Loss: 1.6905 Acc: 0.5600\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train Loss: 1.7183 Acc: 0.4500\n",
      "val Loss: 1.6567 Acc: 0.5600\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train Loss: 1.6904 Acc: 0.4500\n",
      "val Loss: 1.6229 Acc: 0.5600\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train Loss: 1.6631 Acc: 0.4375\n",
      "val Loss: 1.5883 Acc: 0.5600\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train Loss: 1.6350 Acc: 0.4375\n",
      "val Loss: 1.5528 Acc: 0.5600\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train Loss: 1.6061 Acc: 0.4500\n",
      "val Loss: 1.5158 Acc: 0.5600\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train Loss: 1.5770 Acc: 0.4500\n",
      "val Loss: 1.4787 Acc: 0.5600\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train Loss: 1.5452 Acc: 0.4500\n",
      "val Loss: 1.4423 Acc: 0.5600\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train Loss: 1.5161 Acc: 0.4500\n",
      "val Loss: 1.4069 Acc: 0.6000\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train Loss: 1.4866 Acc: 0.4875\n",
      "val Loss: 1.3729 Acc: 0.6800\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train Loss: 1.4591 Acc: 0.5125\n",
      "val Loss: 1.3411 Acc: 0.6800\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train Loss: 1.4305 Acc: 0.5250\n",
      "val Loss: 1.3118 Acc: 0.6800\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train Loss: 1.4042 Acc: 0.5375\n",
      "val Loss: 1.2846 Acc: 0.6800\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train Loss: 1.3808 Acc: 0.5750\n",
      "val Loss: 1.2568 Acc: 0.6800\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train Loss: 1.3535 Acc: 0.5750\n",
      "val Loss: 1.2332 Acc: 0.6800\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train Loss: 1.3293 Acc: 0.5875\n",
      "val Loss: 1.2089 Acc: 0.7200\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train Loss: 1.3046 Acc: 0.5875\n",
      "val Loss: 1.1853 Acc: 0.7200\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train Loss: 1.2804 Acc: 0.5875\n",
      "val Loss: 1.1636 Acc: 0.7200\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train Loss: 1.2560 Acc: 0.5875\n",
      "val Loss: 1.1397 Acc: 0.7200\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train Loss: 1.2299 Acc: 0.5875\n",
      "val Loss: 1.1133 Acc: 0.7200\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train Loss: 1.2051 Acc: 0.5875\n",
      "val Loss: 1.0925 Acc: 0.7200\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train Loss: 1.1775 Acc: 0.6125\n",
      "val Loss: 1.0716 Acc: 0.7200\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train Loss: 1.1500 Acc: 0.6250\n",
      "val Loss: 1.0499 Acc: 0.7200\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train Loss: 1.1220 Acc: 0.6250\n",
      "val Loss: 1.0338 Acc: 0.7200\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train Loss: 1.0948 Acc: 0.6375\n",
      "val Loss: 1.0210 Acc: 0.7200\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train Loss: 1.0684 Acc: 0.6625\n",
      "val Loss: 0.9937 Acc: 0.7600\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train Loss: 1.0438 Acc: 0.6625\n",
      "val Loss: 0.9746 Acc: 0.7600\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train Loss: 1.0174 Acc: 0.6750\n",
      "val Loss: 0.9577 Acc: 0.8000\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train Loss: 1.0001 Acc: 0.7125\n",
      "val Loss: 0.9371 Acc: 0.8000\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train Loss: 0.9748 Acc: 0.7125\n",
      "val Loss: 0.9039 Acc: 0.8000\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train Loss: 0.9506 Acc: 0.7250\n",
      "val Loss: 0.8909 Acc: 0.8000\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train Loss: 0.9224 Acc: 0.7625\n",
      "val Loss: 0.8869 Acc: 0.8400\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train Loss: 0.8985 Acc: 0.8000\n",
      "val Loss: 0.8827 Acc: 0.8000\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train Loss: 0.8743 Acc: 0.8000\n",
      "val Loss: 0.8651 Acc: 0.8000\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train Loss: 0.8505 Acc: 0.7875\n",
      "val Loss: 0.8498 Acc: 0.8000\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train Loss: 0.8273 Acc: 0.8000\n",
      "val Loss: 0.8392 Acc: 0.8000\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train Loss: 0.8008 Acc: 0.8000\n",
      "val Loss: 0.8214 Acc: 0.8000\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train Loss: 0.7774 Acc: 0.8000\n",
      "val Loss: 0.8030 Acc: 0.8000\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train Loss: 0.7528 Acc: 0.7875\n",
      "val Loss: 0.7845 Acc: 0.8000\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train Loss: 0.7383 Acc: 0.7875\n",
      "val Loss: 0.7743 Acc: 0.8000\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train Loss: 0.7078 Acc: 0.8000\n",
      "val Loss: 0.7785 Acc: 0.8000\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train Loss: 0.7087 Acc: 0.8000\n",
      "val Loss: 0.7300 Acc: 0.8400\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.8000\n",
      "val Loss: 0.7311 Acc: 0.8000\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train Loss: 0.6690 Acc: 0.8125\n",
      "val Loss: 0.7325 Acc: 0.8000\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train Loss: 0.6494 Acc: 0.8375\n",
      "val Loss: 0.7264 Acc: 0.8000\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.8500\n",
      "val Loss: 0.7260 Acc: 0.8000\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train Loss: 0.6064 Acc: 0.8625\n",
      "val Loss: 0.7055 Acc: 0.8000\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.8750\n",
      "val Loss: 0.6942 Acc: 0.8400\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train Loss: 0.5645 Acc: 0.8875\n",
      "val Loss: 0.7149 Acc: 0.8400\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train Loss: 0.5510 Acc: 0.9000\n",
      "val Loss: 0.7079 Acc: 0.8400\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train Loss: 0.5254 Acc: 0.9250\n",
      "val Loss: 0.6992 Acc: 0.8400\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train Loss: 0.5061 Acc: 0.9500\n",
      "val Loss: 0.6948 Acc: 0.8400\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train Loss: 0.4802 Acc: 0.9500\n",
      "val Loss: 0.6614 Acc: 0.8400\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train Loss: 0.4579 Acc: 0.9500\n",
      "val Loss: 0.6664 Acc: 0.8400\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train Loss: 0.4417 Acc: 0.9375\n",
      "val Loss: 0.6824 Acc: 0.8400\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train Loss: 0.4314 Acc: 0.9375\n",
      "val Loss: 0.6416 Acc: 0.8400\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train Loss: 0.4419 Acc: 0.9125\n",
      "val Loss: 0.6529 Acc: 0.8400\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train Loss: 0.3972 Acc: 0.9625\n",
      "val Loss: 0.7212 Acc: 0.8000\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train Loss: 0.3935 Acc: 0.9625\n",
      "val Loss: 0.6388 Acc: 0.8400\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train Loss: 0.4070 Acc: 0.9375\n",
      "val Loss: 0.6290 Acc: 0.8400\n",
      "\n",
      "Epoch 63/200\n",
      "----------\n",
      "train Loss: 0.3717 Acc: 0.9625\n",
      "val Loss: 0.7036 Acc: 0.8000\n",
      "\n",
      "Epoch 64/200\n",
      "----------\n",
      "train Loss: 0.3810 Acc: 0.9625\n",
      "val Loss: 0.6611 Acc: 0.8000\n",
      "\n",
      "Epoch 65/200\n",
      "----------\n",
      "train Loss: 0.3428 Acc: 0.9625\n",
      "val Loss: 0.6009 Acc: 0.8400\n",
      "\n",
      "Epoch 66/200\n",
      "----------\n",
      "train Loss: 0.3896 Acc: 0.9250\n",
      "val Loss: 0.6046 Acc: 0.8400\n",
      "\n",
      "Epoch 67/200\n",
      "----------\n",
      "train Loss: 0.3540 Acc: 0.9750\n",
      "val Loss: 0.6803 Acc: 0.8000\n",
      "\n",
      "Epoch 68/200\n",
      "----------\n",
      "train Loss: 0.3817 Acc: 0.9625\n",
      "val Loss: 0.6070 Acc: 0.8400\n",
      "\n",
      "Epoch 69/200\n",
      "----------\n",
      "train Loss: 0.3432 Acc: 0.9750\n",
      "val Loss: 0.5876 Acc: 0.8400\n",
      "\n",
      "Epoch 70/200\n",
      "----------\n",
      "train Loss: 0.3417 Acc: 0.9375\n",
      "val Loss: 0.6103 Acc: 0.8400\n",
      "\n",
      "Epoch 71/200\n",
      "----------\n",
      "train Loss: 0.3135 Acc: 0.9875\n",
      "val Loss: 0.6770 Acc: 0.8400\n",
      "\n",
      "Epoch 72/200\n",
      "----------\n",
      "train Loss: 0.3371 Acc: 0.9500\n",
      "val Loss: 0.6583 Acc: 0.8000\n",
      "\n",
      "Epoch 73/200\n",
      "----------\n",
      "train Loss: 0.3140 Acc: 0.9625\n",
      "val Loss: 0.6685 Acc: 0.8000\n",
      "\n",
      "Epoch 74/200\n",
      "----------\n",
      "train Loss: 0.2950 Acc: 0.9875\n",
      "val Loss: 0.6709 Acc: 0.8000\n",
      "\n",
      "Epoch 75/200\n",
      "----------\n",
      "train Loss: 0.2827 Acc: 0.9875\n",
      "val Loss: 0.6451 Acc: 0.8400\n",
      "\n",
      "Epoch 76/200\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.9875\n",
      "val Loss: 0.6497 Acc: 0.8000\n",
      "\n",
      "Epoch 77/200\n",
      "----------\n",
      "train Loss: 0.2539 Acc: 0.9875\n",
      "val Loss: 0.6645 Acc: 0.7600\n",
      "\n",
      "Epoch 78/200\n",
      "----------\n",
      "train Loss: 0.2462 Acc: 0.9875\n",
      "val Loss: 0.6674 Acc: 0.7600\n",
      "\n",
      "Epoch 79/200\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.9875\n",
      "val Loss: 0.6433 Acc: 0.8000\n",
      "\n",
      "Epoch 80/200\n",
      "----------\n",
      "train Loss: 0.2315 Acc: 0.9875\n",
      "val Loss: 0.6353 Acc: 0.8000\n",
      "\n",
      "Epoch 81/200\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9875\n",
      "val Loss: 0.6315 Acc: 0.8000\n",
      "\n",
      "Epoch 82/200\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9875\n",
      "val Loss: 0.6392 Acc: 0.8000\n",
      "\n",
      "Epoch 83/200\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9875\n",
      "val Loss: 0.6699 Acc: 0.7200\n",
      "\n",
      "Epoch 84/200\n",
      "----------\n",
      "train Loss: 0.2057 Acc: 0.9875\n",
      "val Loss: 0.7252 Acc: 0.7200\n",
      "\n",
      "Epoch 85/200\n",
      "----------\n",
      "train Loss: 0.2001 Acc: 0.9875\n",
      "val Loss: 0.7271 Acc: 0.7200\n",
      "\n",
      "Epoch 86/200\n",
      "----------\n",
      "train Loss: 0.1928 Acc: 0.9875\n",
      "val Loss: 0.6684 Acc: 0.8000\n",
      "\n",
      "Epoch 87/200\n",
      "----------\n",
      "train Loss: 0.1886 Acc: 0.9875\n",
      "val Loss: 0.6177 Acc: 0.8400\n",
      "\n",
      "Epoch 88/200\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.9875\n",
      "val Loss: 0.7103 Acc: 0.7600\n",
      "\n",
      "Epoch 89/200\n",
      "----------\n",
      "train Loss: 0.1769 Acc: 0.9875\n",
      "val Loss: 0.7087 Acc: 0.7600\n",
      "\n",
      "Epoch 90/200\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9875\n",
      "val Loss: 0.6947 Acc: 0.7600\n",
      "\n",
      "Epoch 91/200\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9875\n",
      "val Loss: 0.6518 Acc: 0.7600\n",
      "\n",
      "Epoch 92/200\n",
      "----------\n",
      "train Loss: 0.1635 Acc: 0.9875\n",
      "val Loss: 0.6282 Acc: 0.8000\n",
      "\n",
      "Epoch 93/200\n",
      "----------\n",
      "train Loss: 0.1595 Acc: 0.9875\n",
      "val Loss: 0.6533 Acc: 0.7600\n",
      "\n",
      "Epoch 94/200\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.9875\n",
      "val Loss: 0.6951 Acc: 0.7600\n",
      "\n",
      "Epoch 95/200\n",
      "----------\n",
      "train Loss: 0.1505 Acc: 0.9875\n",
      "val Loss: 0.7076 Acc: 0.7600\n",
      "\n",
      "Epoch 96/200\n",
      "----------\n",
      "train Loss: 0.1468 Acc: 0.9875\n",
      "val Loss: 0.7235 Acc: 0.7600\n",
      "\n",
      "Epoch 97/200\n",
      "----------\n",
      "train Loss: 0.1434 Acc: 0.9875\n",
      "val Loss: 0.7301 Acc: 0.7600\n",
      "\n",
      "Epoch 98/200\n",
      "----------\n",
      "train Loss: 0.1400 Acc: 0.9875\n",
      "val Loss: 0.7235 Acc: 0.7600\n",
      "\n",
      "Epoch 99/200\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9875\n",
      "val Loss: 0.7171 Acc: 0.7600\n",
      "\n",
      "Epoch 100/200\n",
      "----------\n",
      "train Loss: 0.1330 Acc: 0.9875\n",
      "val Loss: 0.7134 Acc: 0.7600\n",
      "\n",
      "Epoch 101/200\n",
      "----------\n",
      "train Loss: 0.1297 Acc: 0.9875\n",
      "val Loss: 0.7099 Acc: 0.7600\n",
      "\n",
      "Epoch 102/200\n",
      "----------\n",
      "train Loss: 0.1268 Acc: 0.9875\n",
      "val Loss: 0.6918 Acc: 0.7600\n",
      "\n",
      "Epoch 103/200\n",
      "----------\n",
      "train Loss: 0.1233 Acc: 0.9875\n",
      "val Loss: 0.6499 Acc: 0.8000\n",
      "\n",
      "Epoch 104/200\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 1.0000\n",
      "val Loss: 0.6540 Acc: 0.7600\n",
      "\n",
      "Epoch 105/200\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 1.0000\n",
      "val Loss: 0.6783 Acc: 0.7600\n",
      "\n",
      "Epoch 106/200\n",
      "----------\n",
      "train Loss: 0.1144 Acc: 1.0000\n",
      "val Loss: 0.7086 Acc: 0.7600\n",
      "\n",
      "Epoch 107/200\n",
      "----------\n",
      "train Loss: 0.1116 Acc: 1.0000\n",
      "val Loss: 0.7144 Acc: 0.7600\n",
      "\n",
      "Epoch 108/200\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 1.0000\n",
      "val Loss: 0.7007 Acc: 0.7600\n",
      "\n",
      "Epoch 109/200\n",
      "----------\n",
      "train Loss: 0.1065 Acc: 1.0000\n",
      "val Loss: 0.6864 Acc: 0.7600\n",
      "\n",
      "Epoch 110/200\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 1.0000\n",
      "val Loss: 0.6876 Acc: 0.7600\n",
      "\n",
      "Epoch 111/200\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 1.0000\n",
      "val Loss: 0.6844 Acc: 0.7600\n",
      "\n",
      "Epoch 112/200\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 1.0000\n",
      "val Loss: 0.6769 Acc: 0.7600\n",
      "\n",
      "Epoch 113/200\n",
      "----------\n",
      "train Loss: 0.0970 Acc: 1.0000\n",
      "val Loss: 0.6724 Acc: 0.7600\n",
      "\n",
      "Epoch 114/200\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 1.0000\n",
      "val Loss: 0.6844 Acc: 0.7600\n",
      "\n",
      "Epoch 115/200\n",
      "----------\n",
      "train Loss: 0.0925 Acc: 1.0000\n",
      "val Loss: 0.6830 Acc: 0.7600\n",
      "\n",
      "Epoch 116/200\n",
      "----------\n",
      "train Loss: 0.0904 Acc: 1.0000\n",
      "val Loss: 0.6748 Acc: 0.7600\n",
      "\n",
      "Epoch 117/200\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 1.0000\n",
      "val Loss: 0.6703 Acc: 0.7600\n",
      "\n",
      "Epoch 118/200\n",
      "----------\n",
      "train Loss: 0.0866 Acc: 1.0000\n",
      "val Loss: 0.6703 Acc: 0.7600\n",
      "\n",
      "Epoch 119/200\n",
      "----------\n",
      "train Loss: 0.0847 Acc: 1.0000\n",
      "val Loss: 0.6804 Acc: 0.7600\n",
      "\n",
      "Epoch 120/200\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 1.0000\n",
      "val Loss: 0.6889 Acc: 0.7600\n",
      "\n",
      "Epoch 121/200\n",
      "----------\n",
      "train Loss: 0.0812 Acc: 1.0000\n",
      "val Loss: 0.6910 Acc: 0.7600\n",
      "\n",
      "Epoch 122/200\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 1.0000\n",
      "val Loss: 0.6888 Acc: 0.7600\n",
      "\n",
      "Epoch 123/200\n",
      "----------\n",
      "train Loss: 0.0780 Acc: 1.0000\n",
      "val Loss: 0.6867 Acc: 0.7600\n",
      "\n",
      "Epoch 124/200\n",
      "----------\n",
      "train Loss: 0.0764 Acc: 1.0000\n",
      "val Loss: 0.6908 Acc: 0.7600\n",
      "\n",
      "Epoch 125/200\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 1.0000\n",
      "val Loss: 0.6938 Acc: 0.7600\n",
      "\n",
      "Epoch 126/200\n",
      "----------\n",
      "train Loss: 0.0735 Acc: 1.0000\n",
      "val Loss: 0.6948 Acc: 0.7600\n",
      "\n",
      "Epoch 127/200\n",
      "----------\n",
      "train Loss: 0.0721 Acc: 1.0000\n",
      "val Loss: 0.6948 Acc: 0.7600\n",
      "\n",
      "Epoch 128/200\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 1.0000\n",
      "val Loss: 0.7012 Acc: 0.7600\n",
      "\n",
      "Epoch 129/200\n",
      "----------\n",
      "train Loss: 0.0694 Acc: 1.0000\n",
      "val Loss: 0.6980 Acc: 0.7600\n",
      "\n",
      "Epoch 130/200\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 1.0000\n",
      "val Loss: 0.6936 Acc: 0.7600\n",
      "\n",
      "Epoch 131/200\n",
      "----------\n",
      "train Loss: 0.0669 Acc: 1.0000\n",
      "val Loss: 0.6900 Acc: 0.7600\n",
      "\n",
      "Epoch 132/200\n",
      "----------\n",
      "train Loss: 0.0658 Acc: 1.0000\n",
      "val Loss: 0.6923 Acc: 0.7600\n",
      "\n",
      "Epoch 133/200\n",
      "----------\n",
      "train Loss: 0.0646 Acc: 1.0000\n",
      "val Loss: 0.6989 Acc: 0.7600\n",
      "\n",
      "Epoch 134/200\n",
      "----------\n",
      "train Loss: 0.0635 Acc: 1.0000\n",
      "val Loss: 0.7077 Acc: 0.7600\n",
      "\n",
      "Epoch 135/200\n",
      "----------\n",
      "train Loss: 0.0625 Acc: 1.0000\n",
      "val Loss: 0.7116 Acc: 0.7600\n",
      "\n",
      "Epoch 136/200\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 1.0000\n",
      "val Loss: 0.7081 Acc: 0.7600\n",
      "\n",
      "Epoch 137/200\n",
      "----------\n",
      "train Loss: 0.0604 Acc: 1.0000\n",
      "val Loss: 0.7065 Acc: 0.7600\n",
      "\n",
      "Epoch 138/200\n",
      "----------\n",
      "train Loss: 0.0594 Acc: 1.0000\n",
      "val Loss: 0.7046 Acc: 0.7600\n",
      "\n",
      "Epoch 139/200\n",
      "----------\n",
      "train Loss: 0.0585 Acc: 1.0000\n",
      "val Loss: 0.7055 Acc: 0.7600\n",
      "\n",
      "Epoch 140/200\n",
      "----------\n",
      "train Loss: 0.0575 Acc: 1.0000\n",
      "val Loss: 0.7084 Acc: 0.7600\n",
      "\n",
      "Epoch 141/200\n",
      "----------\n",
      "train Loss: 0.0566 Acc: 1.0000\n",
      "val Loss: 0.7115 Acc: 0.7600\n",
      "\n",
      "Epoch 142/200\n",
      "----------\n",
      "train Loss: 0.0557 Acc: 1.0000\n",
      "val Loss: 0.7122 Acc: 0.7600\n",
      "\n",
      "Epoch 143/200\n",
      "----------\n",
      "train Loss: 0.0548 Acc: 1.0000\n",
      "val Loss: 0.7137 Acc: 0.7600\n",
      "\n",
      "Epoch 144/200\n",
      "----------\n",
      "train Loss: 0.0540 Acc: 1.0000\n",
      "val Loss: 0.7100 Acc: 0.7600\n",
      "\n",
      "Epoch 145/200\n",
      "----------\n",
      "train Loss: 0.0532 Acc: 1.0000\n",
      "val Loss: 0.7067 Acc: 0.7600\n",
      "\n",
      "Epoch 146/200\n",
      "----------\n",
      "train Loss: 0.0524 Acc: 1.0000\n",
      "val Loss: 0.7056 Acc: 0.7600\n",
      "\n",
      "Epoch 147/200\n",
      "----------\n",
      "train Loss: 0.0516 Acc: 1.0000\n",
      "val Loss: 0.7065 Acc: 0.7600\n",
      "\n",
      "Epoch 148/200\n",
      "----------\n",
      "train Loss: 0.0509 Acc: 1.0000\n",
      "val Loss: 0.7059 Acc: 0.7600\n",
      "\n",
      "Epoch 149/200\n",
      "----------\n",
      "train Loss: 0.0502 Acc: 1.0000\n",
      "val Loss: 0.7055 Acc: 0.7600\n",
      "\n",
      "Epoch 150/200\n",
      "----------\n",
      "train Loss: 0.0495 Acc: 1.0000\n",
      "val Loss: 0.7038 Acc: 0.7600\n",
      "\n",
      "Epoch 151/200\n",
      "----------\n",
      "train Loss: 0.0488 Acc: 1.0000\n",
      "val Loss: 0.7000 Acc: 0.7600\n",
      "\n",
      "Epoch 152/200\n",
      "----------\n",
      "train Loss: 0.0480 Acc: 1.0000\n",
      "val Loss: 0.6967 Acc: 0.7600\n",
      "\n",
      "Epoch 153/200\n",
      "----------\n",
      "train Loss: 0.0474 Acc: 1.0000\n",
      "val Loss: 0.6878 Acc: 0.7600\n",
      "\n",
      "Epoch 154/200\n",
      "----------\n",
      "train Loss: 0.0467 Acc: 1.0000\n",
      "val Loss: 0.6790 Acc: 0.7600\n",
      "\n",
      "Epoch 155/200\n",
      "----------\n",
      "train Loss: 0.0460 Acc: 1.0000\n",
      "val Loss: 0.6744 Acc: 0.8000\n",
      "\n",
      "Epoch 156/200\n",
      "----------\n",
      "train Loss: 0.0453 Acc: 1.0000\n",
      "val Loss: 0.6701 Acc: 0.8000\n",
      "\n",
      "Epoch 157/200\n",
      "----------\n",
      "train Loss: 0.0447 Acc: 1.0000\n",
      "val Loss: 0.6637 Acc: 0.8000\n",
      "\n",
      "Epoch 158/200\n",
      "----------\n",
      "train Loss: 0.0441 Acc: 1.0000\n",
      "val Loss: 0.6587 Acc: 0.8000\n",
      "\n",
      "Epoch 159/200\n",
      "----------\n",
      "train Loss: 0.0434 Acc: 1.0000\n",
      "val Loss: 0.6577 Acc: 0.8000\n",
      "\n",
      "Epoch 160/200\n",
      "----------\n",
      "train Loss: 0.0429 Acc: 1.0000\n",
      "val Loss: 0.6598 Acc: 0.8000\n",
      "\n",
      "Epoch 161/200\n",
      "----------\n",
      "train Loss: 0.0422 Acc: 1.0000\n",
      "val Loss: 0.6691 Acc: 0.8000\n",
      "\n",
      "Epoch 162/200\n",
      "----------\n",
      "train Loss: 0.0416 Acc: 1.0000\n",
      "val Loss: 0.6702 Acc: 0.8000\n",
      "\n",
      "Epoch 163/200\n",
      "----------\n",
      "train Loss: 0.0410 Acc: 1.0000\n",
      "val Loss: 0.6722 Acc: 0.8000\n",
      "\n",
      "Epoch 164/200\n",
      "----------\n",
      "train Loss: 0.0405 Acc: 1.0000\n",
      "val Loss: 0.6707 Acc: 0.8000\n",
      "\n",
      "Epoch 165/200\n",
      "----------\n",
      "train Loss: 0.0399 Acc: 1.0000\n",
      "val Loss: 0.6621 Acc: 0.8000\n",
      "\n",
      "Epoch 166/200\n",
      "----------\n",
      "train Loss: 0.0394 Acc: 1.0000\n",
      "val Loss: 0.6607 Acc: 0.8000\n",
      "\n",
      "Epoch 167/200\n",
      "----------\n",
      "train Loss: 0.0389 Acc: 1.0000\n",
      "val Loss: 0.6613 Acc: 0.8000\n",
      "\n",
      "Epoch 168/200\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 1.0000\n",
      "val Loss: 0.6635 Acc: 0.8000\n",
      "\n",
      "Epoch 169/200\n",
      "----------\n",
      "train Loss: 0.0379 Acc: 1.0000\n",
      "val Loss: 0.6645 Acc: 0.8000\n",
      "\n",
      "Epoch 170/200\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 1.0000\n",
      "val Loss: 0.6637 Acc: 0.8000\n",
      "\n",
      "Epoch 171/200\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 1.0000\n",
      "val Loss: 0.6647 Acc: 0.8000\n",
      "\n",
      "Epoch 172/200\n",
      "----------\n",
      "train Loss: 0.0365 Acc: 1.0000\n",
      "val Loss: 0.6674 Acc: 0.8000\n",
      "\n",
      "Epoch 173/200\n",
      "----------\n",
      "train Loss: 0.0361 Acc: 1.0000\n",
      "val Loss: 0.6736 Acc: 0.8000\n",
      "\n",
      "Epoch 174/200\n",
      "----------\n",
      "train Loss: 0.0356 Acc: 1.0000\n",
      "val Loss: 0.6761 Acc: 0.8000\n",
      "\n",
      "Epoch 175/200\n",
      "----------\n",
      "train Loss: 0.0352 Acc: 1.0000\n",
      "val Loss: 0.6761 Acc: 0.8000\n",
      "\n",
      "Epoch 176/200\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 1.0000\n",
      "val Loss: 0.6767 Acc: 0.8000\n",
      "\n",
      "Epoch 177/200\n",
      "----------\n",
      "train Loss: 0.0344 Acc: 1.0000\n",
      "val Loss: 0.6775 Acc: 0.8000\n",
      "\n",
      "Epoch 178/200\n",
      "----------\n",
      "train Loss: 0.0340 Acc: 1.0000\n",
      "val Loss: 0.6801 Acc: 0.8000\n",
      "\n",
      "Epoch 179/200\n",
      "----------\n",
      "train Loss: 0.0336 Acc: 1.0000\n",
      "val Loss: 0.6824 Acc: 0.8000\n",
      "\n",
      "Epoch 180/200\n",
      "----------\n",
      "train Loss: 0.0332 Acc: 1.0000\n",
      "val Loss: 0.6809 Acc: 0.8000\n",
      "\n",
      "Epoch 181/200\n",
      "----------\n",
      "train Loss: 0.0328 Acc: 1.0000\n",
      "val Loss: 0.6829 Acc: 0.8000\n",
      "\n",
      "Epoch 182/200\n",
      "----------\n",
      "train Loss: 0.0324 Acc: 1.0000\n",
      "val Loss: 0.6857 Acc: 0.8000\n",
      "\n",
      "Epoch 183/200\n",
      "----------\n",
      "train Loss: 0.0321 Acc: 1.0000\n",
      "val Loss: 0.6865 Acc: 0.8000\n",
      "\n",
      "Epoch 184/200\n",
      "----------\n",
      "train Loss: 0.0317 Acc: 1.0000\n",
      "val Loss: 0.6846 Acc: 0.8000\n",
      "\n",
      "Epoch 185/200\n",
      "----------\n",
      "train Loss: 0.0314 Acc: 1.0000\n",
      "val Loss: 0.6827 Acc: 0.8000\n",
      "\n",
      "Epoch 186/200\n",
      "----------\n",
      "train Loss: 0.0310 Acc: 1.0000\n",
      "val Loss: 0.6822 Acc: 0.8000\n",
      "\n",
      "Epoch 187/200\n",
      "----------\n",
      "train Loss: 0.0307 Acc: 1.0000\n",
      "val Loss: 0.6832 Acc: 0.8000\n",
      "\n",
      "Epoch 188/200\n",
      "----------\n",
      "train Loss: 0.0303 Acc: 1.0000\n",
      "val Loss: 0.6854 Acc: 0.8000\n",
      "\n",
      "Epoch 189/200\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 1.0000\n",
      "val Loss: 0.6864 Acc: 0.8000\n",
      "\n",
      "Epoch 190/200\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 1.0000\n",
      "val Loss: 0.6880 Acc: 0.8000\n",
      "\n",
      "Epoch 191/200\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 1.0000\n",
      "val Loss: 0.6906 Acc: 0.8000\n",
      "\n",
      "Epoch 192/200\n",
      "----------\n",
      "train Loss: 0.0290 Acc: 1.0000\n",
      "val Loss: 0.6951 Acc: 0.8000\n",
      "\n",
      "Epoch 193/200\n",
      "----------\n",
      "train Loss: 0.0287 Acc: 1.0000\n",
      "val Loss: 0.7012 Acc: 0.8000\n",
      "\n",
      "Epoch 194/200\n",
      "----------\n",
      "train Loss: 0.0284 Acc: 1.0000\n",
      "val Loss: 0.7047 Acc: 0.8000\n",
      "\n",
      "Epoch 195/200\n",
      "----------\n",
      "train Loss: 0.0281 Acc: 1.0000\n",
      "val Loss: 0.7045 Acc: 0.8000\n",
      "\n",
      "Epoch 196/200\n",
      "----------\n",
      "train Loss: 0.0278 Acc: 1.0000\n",
      "val Loss: 0.7045 Acc: 0.8000\n",
      "\n",
      "Epoch 197/200\n",
      "----------\n",
      "train Loss: 0.0275 Acc: 1.0000\n",
      "val Loss: 0.7044 Acc: 0.8000\n",
      "\n",
      "Epoch 198/200\n",
      "----------\n",
      "train Loss: 0.0272 Acc: 1.0000\n",
      "val Loss: 0.7056 Acc: 0.8000\n",
      "\n",
      "Epoch 199/200\n",
      "----------\n",
      "train Loss: 0.0269 Acc: 1.0000\n",
      "val Loss: 0.7072 Acc: 0.8000\n",
      "\n",
      "Epoch 200/200\n",
      "----------\n",
      "train Loss: 0.0267 Acc: 1.0000\n",
      "val Loss: 0.7084 Acc: 0.8000\n",
      "\n",
      "Training complete in 1m 15s\n",
      "Best val Acc: 0.840000\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 학습\n",
    "lstm, lstm_val_acc_history = train_model(lstm, dataloaders_dict, criterion, num_epochs,\n",
    "                                         optimizer=optim.Adam(lstm.parameters(), lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:46.415128Z",
     "iopub.status.busy": "2022-01-10T12:00:46.414788Z",
     "iopub.status.idle": "2022-01-10T12:00:53.326613Z",
     "shell.execute_reply": "2022-01-10T12:00:53.325833Z",
     "shell.execute_reply.started": "2022-01-10T12:00:46.415089Z"
    },
    "id": "YpPqA3iG8RpP",
    "outputId": "ed64a158-6552-437c-baf2-813bc58b266d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n",
      "train Loss: 1.7324 Acc: 0.3750\n",
      "val Loss: 1.6347 Acc: 0.5200\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train Loss: 1.6366 Acc: 0.4500\n",
      "val Loss: 1.5424 Acc: 0.5200\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train Loss: 1.5529 Acc: 0.4375\n",
      "val Loss: 1.4709 Acc: 0.5200\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train Loss: 1.4876 Acc: 0.4375\n",
      "val Loss: 1.4117 Acc: 0.5200\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train Loss: 1.4310 Acc: 0.4375\n",
      "val Loss: 1.3663 Acc: 0.5200\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train Loss: 1.3864 Acc: 0.4500\n",
      "val Loss: 1.3303 Acc: 0.5200\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train Loss: 1.3492 Acc: 0.4500\n",
      "val Loss: 1.2997 Acc: 0.5200\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train Loss: 1.3191 Acc: 0.4500\n",
      "val Loss: 1.2753 Acc: 0.5200\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train Loss: 1.2923 Acc: 0.4625\n",
      "val Loss: 1.2574 Acc: 0.5200\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train Loss: 1.2670 Acc: 0.5125\n",
      "val Loss: 1.2389 Acc: 0.5600\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train Loss: 1.2449 Acc: 0.5625\n",
      "val Loss: 1.2190 Acc: 0.6000\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train Loss: 1.2255 Acc: 0.5375\n",
      "val Loss: 1.2013 Acc: 0.6400\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train Loss: 1.2071 Acc: 0.5375\n",
      "val Loss: 1.1847 Acc: 0.6800\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train Loss: 1.1884 Acc: 0.5750\n",
      "val Loss: 1.1705 Acc: 0.7200\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train Loss: 1.1713 Acc: 0.6125\n",
      "val Loss: 1.1561 Acc: 0.6800\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train Loss: 1.1564 Acc: 0.6250\n",
      "val Loss: 1.1420 Acc: 0.6800\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train Loss: 1.1403 Acc: 0.6250\n",
      "val Loss: 1.1290 Acc: 0.6800\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train Loss: 1.1246 Acc: 0.6375\n",
      "val Loss: 1.1165 Acc: 0.6800\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train Loss: 1.1096 Acc: 0.6250\n",
      "val Loss: 1.1029 Acc: 0.6800\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.6250\n",
      "val Loss: 1.0914 Acc: 0.6800\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train Loss: 1.0825 Acc: 0.6250\n",
      "val Loss: 1.0790 Acc: 0.6800\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train Loss: 1.0684 Acc: 0.6375\n",
      "val Loss: 1.0667 Acc: 0.7200\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train Loss: 1.0550 Acc: 0.6250\n",
      "val Loss: 1.0529 Acc: 0.7200\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train Loss: 1.0418 Acc: 0.6250\n",
      "val Loss: 1.0406 Acc: 0.7200\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train Loss: 1.0289 Acc: 0.6375\n",
      "val Loss: 1.0321 Acc: 0.7200\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train Loss: 1.0152 Acc: 0.6375\n",
      "val Loss: 1.0239 Acc: 0.7200\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train Loss: 1.0022 Acc: 0.6375\n",
      "val Loss: 1.0132 Acc: 0.7600\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train Loss: 0.9895 Acc: 0.6500\n",
      "val Loss: 1.0034 Acc: 0.7600\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train Loss: 0.9767 Acc: 0.6500\n",
      "val Loss: 0.9930 Acc: 0.7600\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.6625\n",
      "val Loss: 0.9859 Acc: 0.7600\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train Loss: 0.9512 Acc: 0.6625\n",
      "val Loss: 0.9803 Acc: 0.7600\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train Loss: 0.9383 Acc: 0.6625\n",
      "val Loss: 0.9741 Acc: 0.7600\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train Loss: 0.9274 Acc: 0.6750\n",
      "val Loss: 0.9670 Acc: 0.7600\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train Loss: 0.9146 Acc: 0.6750\n",
      "val Loss: 0.9541 Acc: 0.7600\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train Loss: 0.9032 Acc: 0.6750\n",
      "val Loss: 0.9402 Acc: 0.7600\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train Loss: 0.8901 Acc: 0.6750\n",
      "val Loss: 0.9277 Acc: 0.7600\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train Loss: 0.8789 Acc: 0.6750\n",
      "val Loss: 0.9105 Acc: 0.7600\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train Loss: 0.8665 Acc: 0.6750\n",
      "val Loss: 0.9002 Acc: 0.7600\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train Loss: 0.8549 Acc: 0.6875\n",
      "val Loss: 0.8971 Acc: 0.7600\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train Loss: 0.8416 Acc: 0.6875\n",
      "val Loss: 0.8892 Acc: 0.7600\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train Loss: 0.8298 Acc: 0.7000\n",
      "val Loss: 0.8786 Acc: 0.7600\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train Loss: 0.8180 Acc: 0.7125\n",
      "val Loss: 0.8723 Acc: 0.7600\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train Loss: 0.8059 Acc: 0.7375\n",
      "val Loss: 0.8687 Acc: 0.7600\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train Loss: 0.7927 Acc: 0.7375\n",
      "val Loss: 0.8614 Acc: 0.7600\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train Loss: 0.7808 Acc: 0.7375\n",
      "val Loss: 0.8567 Acc: 0.7600\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train Loss: 0.7693 Acc: 0.7500\n",
      "val Loss: 0.8485 Acc: 0.7600\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train Loss: 0.7565 Acc: 0.7875\n",
      "val Loss: 0.8365 Acc: 0.7600\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train Loss: 0.7464 Acc: 0.7875\n",
      "val Loss: 0.8191 Acc: 0.7600\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train Loss: 0.7326 Acc: 0.7750\n",
      "val Loss: 0.8111 Acc: 0.7600\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train Loss: 0.7206 Acc: 0.7875\n",
      "val Loss: 0.7995 Acc: 0.7600\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train Loss: 0.7078 Acc: 0.8000\n",
      "val Loss: 0.7912 Acc: 0.7600\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train Loss: 0.6962 Acc: 0.8000\n",
      "val Loss: 0.7815 Acc: 0.7600\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train Loss: 0.6860 Acc: 0.8000\n",
      "val Loss: 0.7747 Acc: 0.8000\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train Loss: 0.6735 Acc: 0.8000\n",
      "val Loss: 0.7657 Acc: 0.8000\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train Loss: 0.6632 Acc: 0.8125\n",
      "val Loss: 0.7621 Acc: 0.8000\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train Loss: 0.6504 Acc: 0.8125\n",
      "val Loss: 0.7492 Acc: 0.8000\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.8125\n",
      "val Loss: 0.7355 Acc: 0.8000\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.8000\n",
      "val Loss: 0.7259 Acc: 0.8000\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.8125\n",
      "val Loss: 0.7236 Acc: 0.8000\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train Loss: 0.6088 Acc: 0.8250\n",
      "val Loss: 0.7228 Acc: 0.8800\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train Loss: 0.5971 Acc: 0.8250\n",
      "val Loss: 0.7143 Acc: 0.8800\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train Loss: 0.5862 Acc: 0.8250\n",
      "val Loss: 0.7050 Acc: 0.8800\n",
      "\n",
      "Epoch 63/200\n",
      "----------\n",
      "train Loss: 0.5763 Acc: 0.8500\n",
      "val Loss: 0.6946 Acc: 0.8400\n",
      "\n",
      "Epoch 64/200\n",
      "----------\n",
      "train Loss: 0.5679 Acc: 0.8500\n",
      "val Loss: 0.6896 Acc: 0.8800\n",
      "\n",
      "Epoch 65/200\n",
      "----------\n",
      "train Loss: 0.5580 Acc: 0.8500\n",
      "val Loss: 0.6849 Acc: 0.8800\n",
      "\n",
      "Epoch 66/200\n",
      "----------\n",
      "train Loss: 0.5481 Acc: 0.8625\n",
      "val Loss: 0.6790 Acc: 0.8800\n",
      "\n",
      "Epoch 67/200\n",
      "----------\n",
      "train Loss: 0.5376 Acc: 0.8625\n",
      "val Loss: 0.6759 Acc: 0.8800\n",
      "\n",
      "Epoch 68/200\n",
      "----------\n",
      "train Loss: 0.5286 Acc: 0.8625\n",
      "val Loss: 0.6784 Acc: 0.8400\n",
      "\n",
      "Epoch 69/200\n",
      "----------\n",
      "train Loss: 0.5200 Acc: 0.8625\n",
      "val Loss: 0.6771 Acc: 0.8400\n",
      "\n",
      "Epoch 70/200\n",
      "----------\n",
      "train Loss: 0.5114 Acc: 0.8500\n",
      "val Loss: 0.6642 Acc: 0.8400\n",
      "\n",
      "Epoch 71/200\n",
      "----------\n",
      "train Loss: 0.5028 Acc: 0.8625\n",
      "val Loss: 0.6504 Acc: 0.8800\n",
      "\n",
      "Epoch 72/200\n",
      "----------\n",
      "train Loss: 0.4929 Acc: 0.8625\n",
      "val Loss: 0.6463 Acc: 0.8800\n",
      "\n",
      "Epoch 73/200\n",
      "----------\n",
      "train Loss: 0.4850 Acc: 0.8625\n",
      "val Loss: 0.6393 Acc: 0.8800\n",
      "\n",
      "Epoch 74/200\n",
      "----------\n",
      "train Loss: 0.4765 Acc: 0.8625\n",
      "val Loss: 0.6354 Acc: 0.8400\n",
      "\n",
      "Epoch 75/200\n",
      "----------\n",
      "train Loss: 0.4688 Acc: 0.8625\n",
      "val Loss: 0.6356 Acc: 0.8000\n",
      "\n",
      "Epoch 76/200\n",
      "----------\n",
      "train Loss: 0.4610 Acc: 0.8625\n",
      "val Loss: 0.6365 Acc: 0.8000\n",
      "\n",
      "Epoch 77/200\n",
      "----------\n",
      "train Loss: 0.4531 Acc: 0.8625\n",
      "val Loss: 0.6314 Acc: 0.8400\n",
      "\n",
      "Epoch 78/200\n",
      "----------\n",
      "train Loss: 0.4450 Acc: 0.8625\n",
      "val Loss: 0.6249 Acc: 0.8400\n",
      "\n",
      "Epoch 79/200\n",
      "----------\n",
      "train Loss: 0.4388 Acc: 0.8625\n",
      "val Loss: 0.6223 Acc: 0.8400\n",
      "\n",
      "Epoch 80/200\n",
      "----------\n",
      "train Loss: 0.4351 Acc: 0.8625\n",
      "val Loss: 0.6099 Acc: 0.8800\n",
      "\n",
      "Epoch 81/200\n",
      "----------\n",
      "train Loss: 0.4242 Acc: 0.8875\n",
      "val Loss: 0.6121 Acc: 0.8000\n",
      "\n",
      "Epoch 82/200\n",
      "----------\n",
      "train Loss: 0.4167 Acc: 0.8750\n",
      "val Loss: 0.6113 Acc: 0.8000\n",
      "\n",
      "Epoch 83/200\n",
      "----------\n",
      "train Loss: 0.4109 Acc: 0.8750\n",
      "val Loss: 0.5995 Acc: 0.8400\n",
      "\n",
      "Epoch 84/200\n",
      "----------\n",
      "train Loss: 0.4029 Acc: 0.8875\n",
      "val Loss: 0.5971 Acc: 0.8400\n",
      "\n",
      "Epoch 85/200\n",
      "----------\n",
      "train Loss: 0.3965 Acc: 0.9000\n",
      "val Loss: 0.6033 Acc: 0.8400\n",
      "\n",
      "Epoch 86/200\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8875\n",
      "val Loss: 0.6159 Acc: 0.8400\n",
      "\n",
      "Epoch 87/200\n",
      "----------\n",
      "train Loss: 0.3850 Acc: 0.9000\n",
      "val Loss: 0.6066 Acc: 0.8400\n",
      "\n",
      "Epoch 88/200\n",
      "----------\n",
      "train Loss: 0.3778 Acc: 0.9125\n",
      "val Loss: 0.5984 Acc: 0.8400\n",
      "\n",
      "Epoch 89/200\n",
      "----------\n",
      "train Loss: 0.3706 Acc: 0.9125\n",
      "val Loss: 0.5885 Acc: 0.8400\n",
      "\n",
      "Epoch 90/200\n",
      "----------\n",
      "train Loss: 0.3629 Acc: 0.9125\n",
      "val Loss: 0.5781 Acc: 0.8800\n",
      "\n",
      "Epoch 91/200\n",
      "----------\n",
      "train Loss: 0.3587 Acc: 0.9125\n",
      "val Loss: 0.5721 Acc: 0.8800\n",
      "\n",
      "Epoch 92/200\n",
      "----------\n",
      "train Loss: 0.3559 Acc: 0.9125\n",
      "val Loss: 0.5805 Acc: 0.8400\n",
      "\n",
      "Epoch 93/200\n",
      "----------\n",
      "train Loss: 0.3474 Acc: 0.9125\n",
      "val Loss: 0.5716 Acc: 0.8000\n",
      "\n",
      "Epoch 94/200\n",
      "----------\n",
      "train Loss: 0.3396 Acc: 0.9125\n",
      "val Loss: 0.5692 Acc: 0.8000\n",
      "\n",
      "Epoch 95/200\n",
      "----------\n",
      "train Loss: 0.3344 Acc: 0.9125\n",
      "val Loss: 0.5693 Acc: 0.8000\n",
      "\n",
      "Epoch 96/200\n",
      "----------\n",
      "train Loss: 0.3280 Acc: 0.9250\n",
      "val Loss: 0.5809 Acc: 0.8000\n",
      "\n",
      "Epoch 97/200\n",
      "----------\n",
      "train Loss: 0.3229 Acc: 0.9375\n",
      "val Loss: 0.5942 Acc: 0.8000\n",
      "\n",
      "Epoch 98/200\n",
      "----------\n",
      "train Loss: 0.3190 Acc: 0.9375\n",
      "val Loss: 0.5833 Acc: 0.8000\n",
      "\n",
      "Epoch 99/200\n",
      "----------\n",
      "train Loss: 0.3134 Acc: 0.9250\n",
      "val Loss: 0.5675 Acc: 0.8000\n",
      "\n",
      "Epoch 100/200\n",
      "----------\n",
      "train Loss: 0.3074 Acc: 0.9250\n",
      "val Loss: 0.5639 Acc: 0.8400\n",
      "\n",
      "Epoch 101/200\n",
      "----------\n",
      "train Loss: 0.3008 Acc: 0.9375\n",
      "val Loss: 0.5532 Acc: 0.8800\n",
      "\n",
      "Epoch 102/200\n",
      "----------\n",
      "train Loss: 0.2987 Acc: 0.9250\n",
      "val Loss: 0.5524 Acc: 0.8800\n",
      "\n",
      "Epoch 103/200\n",
      "----------\n",
      "train Loss: 0.2941 Acc: 0.9375\n",
      "val Loss: 0.5584 Acc: 0.8400\n",
      "\n",
      "Epoch 104/200\n",
      "----------\n",
      "train Loss: 0.2864 Acc: 0.9375\n",
      "val Loss: 0.5688 Acc: 0.8400\n",
      "\n",
      "Epoch 105/200\n",
      "----------\n",
      "train Loss: 0.2820 Acc: 0.9375\n",
      "val Loss: 0.5776 Acc: 0.8400\n",
      "\n",
      "Epoch 106/200\n",
      "----------\n",
      "train Loss: 0.2773 Acc: 0.9375\n",
      "val Loss: 0.5689 Acc: 0.8400\n",
      "\n",
      "Epoch 107/200\n",
      "----------\n",
      "train Loss: 0.2711 Acc: 0.9250\n",
      "val Loss: 0.5484 Acc: 0.8400\n",
      "\n",
      "Epoch 108/200\n",
      "----------\n",
      "train Loss: 0.2684 Acc: 0.9250\n",
      "val Loss: 0.5470 Acc: 0.8400\n",
      "\n",
      "Epoch 109/200\n",
      "----------\n",
      "train Loss: 0.2624 Acc: 0.9500\n",
      "val Loss: 0.5615 Acc: 0.8400\n",
      "\n",
      "Epoch 110/200\n",
      "----------\n",
      "train Loss: 0.2614 Acc: 0.9500\n",
      "val Loss: 0.5981 Acc: 0.8400\n",
      "\n",
      "Epoch 111/200\n",
      "----------\n",
      "train Loss: 0.2557 Acc: 0.9500\n",
      "val Loss: 0.5748 Acc: 0.8400\n",
      "\n",
      "Epoch 112/200\n",
      "----------\n",
      "train Loss: 0.2477 Acc: 0.9500\n",
      "val Loss: 0.5463 Acc: 0.8400\n",
      "\n",
      "Epoch 113/200\n",
      "----------\n",
      "train Loss: 0.2471 Acc: 0.9500\n",
      "val Loss: 0.5379 Acc: 0.8800\n",
      "\n",
      "Epoch 114/200\n",
      "----------\n",
      "train Loss: 0.2433 Acc: 0.9500\n",
      "val Loss: 0.5492 Acc: 0.8400\n",
      "\n",
      "Epoch 115/200\n",
      "----------\n",
      "train Loss: 0.2355 Acc: 0.9375\n",
      "val Loss: 0.5671 Acc: 0.8400\n",
      "\n",
      "Epoch 116/200\n",
      "----------\n",
      "train Loss: 0.2309 Acc: 0.9500\n",
      "val Loss: 0.5605 Acc: 0.8400\n",
      "\n",
      "Epoch 117/200\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.9625\n",
      "val Loss: 0.5373 Acc: 0.8800\n",
      "\n",
      "Epoch 118/200\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.9500\n",
      "val Loss: 0.5466 Acc: 0.8400\n",
      "\n",
      "Epoch 119/200\n",
      "----------\n",
      "train Loss: 0.2165 Acc: 0.9625\n",
      "val Loss: 0.5612 Acc: 0.8400\n",
      "\n",
      "Epoch 120/200\n",
      "----------\n",
      "train Loss: 0.2125 Acc: 0.9625\n",
      "val Loss: 0.5652 Acc: 0.8400\n",
      "\n",
      "Epoch 121/200\n",
      "----------\n",
      "train Loss: 0.2101 Acc: 0.9625\n",
      "val Loss: 0.5515 Acc: 0.8400\n",
      "\n",
      "Epoch 122/200\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9625\n",
      "val Loss: 0.5635 Acc: 0.8400\n",
      "\n",
      "Epoch 123/200\n",
      "----------\n",
      "train Loss: 0.1997 Acc: 0.9625\n",
      "val Loss: 0.5604 Acc: 0.8400\n",
      "\n",
      "Epoch 124/200\n",
      "----------\n",
      "train Loss: 0.1963 Acc: 0.9625\n",
      "val Loss: 0.5442 Acc: 0.8400\n",
      "\n",
      "Epoch 125/200\n",
      "----------\n",
      "train Loss: 0.1913 Acc: 0.9625\n",
      "val Loss: 0.5326 Acc: 0.8800\n",
      "\n",
      "Epoch 126/200\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.9625\n",
      "val Loss: 0.5412 Acc: 0.8400\n",
      "\n",
      "Epoch 127/200\n",
      "----------\n",
      "train Loss: 0.1839 Acc: 0.9625\n",
      "val Loss: 0.5460 Acc: 0.8400\n",
      "\n",
      "Epoch 128/200\n",
      "----------\n",
      "train Loss: 0.1796 Acc: 0.9750\n",
      "val Loss: 0.5670 Acc: 0.8400\n",
      "\n",
      "Epoch 129/200\n",
      "----------\n",
      "train Loss: 0.1767 Acc: 0.9750\n",
      "val Loss: 0.5608 Acc: 0.8400\n",
      "\n",
      "Epoch 130/200\n",
      "----------\n",
      "train Loss: 0.1719 Acc: 0.9750\n",
      "val Loss: 0.5348 Acc: 0.8400\n",
      "\n",
      "Epoch 131/200\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.9625\n",
      "val Loss: 0.5499 Acc: 0.8400\n",
      "\n",
      "Epoch 132/200\n",
      "----------\n",
      "train Loss: 0.1694 Acc: 0.9625\n",
      "val Loss: 0.5788 Acc: 0.8400\n",
      "\n",
      "Epoch 133/200\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9750\n",
      "val Loss: 0.5568 Acc: 0.8400\n",
      "\n",
      "Epoch 134/200\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.9750\n",
      "val Loss: 0.5665 Acc: 0.8400\n",
      "\n",
      "Epoch 135/200\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9875\n",
      "val Loss: 0.5693 Acc: 0.8400\n",
      "\n",
      "Epoch 136/200\n",
      "----------\n",
      "train Loss: 0.1466 Acc: 0.9875\n",
      "val Loss: 0.5679 Acc: 0.8400\n",
      "\n",
      "Epoch 137/200\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9875\n",
      "val Loss: 0.5601 Acc: 0.8400\n",
      "\n",
      "Epoch 138/200\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9750\n",
      "val Loss: 0.5661 Acc: 0.8400\n",
      "\n",
      "Epoch 139/200\n",
      "----------\n",
      "train Loss: 0.1373 Acc: 1.0000\n",
      "val Loss: 0.6027 Acc: 0.8400\n",
      "\n",
      "Epoch 140/200\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9875\n",
      "val Loss: 0.5606 Acc: 0.8400\n",
      "\n",
      "Epoch 141/200\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9750\n",
      "val Loss: 0.5437 Acc: 0.8400\n",
      "\n",
      "Epoch 142/200\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9875\n",
      "val Loss: 0.5660 Acc: 0.8400\n",
      "\n",
      "Epoch 143/200\n",
      "----------\n",
      "train Loss: 0.1222 Acc: 1.0000\n",
      "val Loss: 0.5920 Acc: 0.8400\n",
      "\n",
      "Epoch 144/200\n",
      "----------\n",
      "train Loss: 0.1189 Acc: 1.0000\n",
      "val Loss: 0.5646 Acc: 0.8400\n",
      "\n",
      "Epoch 145/200\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 1.0000\n",
      "val Loss: 0.5546 Acc: 0.8400\n",
      "\n",
      "Epoch 146/200\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 1.0000\n",
      "val Loss: 0.5496 Acc: 0.8400\n",
      "\n",
      "Epoch 147/200\n",
      "----------\n",
      "train Loss: 0.1057 Acc: 1.0000\n",
      "val Loss: 0.5597 Acc: 0.8400\n",
      "\n",
      "Epoch 148/200\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 1.0000\n",
      "val Loss: 0.5615 Acc: 0.8400\n",
      "\n",
      "Epoch 149/200\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 1.0000\n",
      "val Loss: 0.5718 Acc: 0.8400\n",
      "\n",
      "Epoch 150/200\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 1.0000\n",
      "val Loss: 0.5541 Acc: 0.8400\n",
      "\n",
      "Epoch 151/200\n",
      "----------\n",
      "train Loss: 0.0945 Acc: 1.0000\n",
      "val Loss: 0.5616 Acc: 0.8400\n",
      "\n",
      "Epoch 152/200\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 1.0000\n",
      "val Loss: 0.5550 Acc: 0.8400\n",
      "\n",
      "Epoch 153/200\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 1.0000\n",
      "val Loss: 0.5503 Acc: 0.8400\n",
      "\n",
      "Epoch 154/200\n",
      "----------\n",
      "train Loss: 0.0849 Acc: 1.0000\n",
      "val Loss: 0.5628 Acc: 0.8400\n",
      "\n",
      "Epoch 155/200\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 1.0000\n",
      "val Loss: 0.5701 Acc: 0.8400\n",
      "\n",
      "Epoch 156/200\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 1.0000\n",
      "val Loss: 0.5663 Acc: 0.8400\n",
      "\n",
      "Epoch 157/200\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 1.0000\n",
      "val Loss: 0.5593 Acc: 0.8400\n",
      "\n",
      "Epoch 158/200\n",
      "----------\n",
      "train Loss: 0.0752 Acc: 1.0000\n",
      "val Loss: 0.5696 Acc: 0.8400\n",
      "\n",
      "Epoch 159/200\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 1.0000\n",
      "val Loss: 0.5635 Acc: 0.8400\n",
      "\n",
      "Epoch 160/200\n",
      "----------\n",
      "train Loss: 0.0710 Acc: 1.0000\n",
      "val Loss: 0.5531 Acc: 0.8400\n",
      "\n",
      "Epoch 161/200\n",
      "----------\n",
      "train Loss: 0.0695 Acc: 1.0000\n",
      "val Loss: 0.5493 Acc: 0.8400\n",
      "\n",
      "Epoch 162/200\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 1.0000\n",
      "val Loss: 0.5710 Acc: 0.8400\n",
      "\n",
      "Epoch 163/200\n",
      "----------\n",
      "train Loss: 0.0659 Acc: 1.0000\n",
      "val Loss: 0.5621 Acc: 0.8400\n",
      "\n",
      "Epoch 164/200\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 1.0000\n",
      "val Loss: 0.5759 Acc: 0.8400\n",
      "\n",
      "Epoch 165/200\n",
      "----------\n",
      "train Loss: 0.0627 Acc: 1.0000\n",
      "val Loss: 0.5606 Acc: 0.8400\n",
      "\n",
      "Epoch 166/200\n",
      "----------\n",
      "train Loss: 0.0608 Acc: 1.0000\n",
      "val Loss: 0.5667 Acc: 0.8400\n",
      "\n",
      "Epoch 167/200\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 1.0000\n",
      "val Loss: 0.5604 Acc: 0.8400\n",
      "\n",
      "Epoch 168/200\n",
      "----------\n",
      "train Loss: 0.0579 Acc: 1.0000\n",
      "val Loss: 0.5772 Acc: 0.8400\n",
      "\n",
      "Epoch 169/200\n",
      "----------\n",
      "train Loss: 0.0560 Acc: 1.0000\n",
      "val Loss: 0.5745 Acc: 0.8400\n",
      "\n",
      "Epoch 170/200\n",
      "----------\n",
      "train Loss: 0.0546 Acc: 1.0000\n",
      "val Loss: 0.5651 Acc: 0.8400\n",
      "\n",
      "Epoch 171/200\n",
      "----------\n",
      "train Loss: 0.0537 Acc: 1.0000\n",
      "val Loss: 0.5627 Acc: 0.8400\n",
      "\n",
      "Epoch 172/200\n",
      "----------\n",
      "train Loss: 0.0524 Acc: 1.0000\n",
      "val Loss: 0.5595 Acc: 0.8400\n",
      "\n",
      "Epoch 173/200\n",
      "----------\n",
      "train Loss: 0.0512 Acc: 1.0000\n",
      "val Loss: 0.5754 Acc: 0.8400\n",
      "\n",
      "Epoch 174/200\n",
      "----------\n",
      "train Loss: 0.0498 Acc: 1.0000\n",
      "val Loss: 0.5778 Acc: 0.8400\n",
      "\n",
      "Epoch 175/200\n",
      "----------\n",
      "train Loss: 0.0489 Acc: 1.0000\n",
      "val Loss: 0.5780 Acc: 0.8400\n",
      "\n",
      "Epoch 176/200\n",
      "----------\n",
      "train Loss: 0.0477 Acc: 1.0000\n",
      "val Loss: 0.5856 Acc: 0.8400\n",
      "\n",
      "Epoch 177/200\n",
      "----------\n",
      "train Loss: 0.0467 Acc: 1.0000\n",
      "val Loss: 0.5736 Acc: 0.8400\n",
      "\n",
      "Epoch 178/200\n",
      "----------\n",
      "train Loss: 0.0455 Acc: 1.0000\n",
      "val Loss: 0.5659 Acc: 0.8400\n",
      "\n",
      "Epoch 179/200\n",
      "----------\n",
      "train Loss: 0.0448 Acc: 1.0000\n",
      "val Loss: 0.5738 Acc: 0.8400\n",
      "\n",
      "Epoch 180/200\n",
      "----------\n",
      "train Loss: 0.0437 Acc: 1.0000\n",
      "val Loss: 0.5756 Acc: 0.8400\n",
      "\n",
      "Epoch 181/200\n",
      "----------\n",
      "train Loss: 0.0427 Acc: 1.0000\n",
      "val Loss: 0.5854 Acc: 0.8400\n",
      "\n",
      "Epoch 182/200\n",
      "----------\n",
      "train Loss: 0.0419 Acc: 1.0000\n",
      "val Loss: 0.5872 Acc: 0.8400\n",
      "\n",
      "Epoch 183/200\n",
      "----------\n",
      "train Loss: 0.0411 Acc: 1.0000\n",
      "val Loss: 0.5790 Acc: 0.8400\n",
      "\n",
      "Epoch 184/200\n",
      "----------\n",
      "train Loss: 0.0402 Acc: 1.0000\n",
      "val Loss: 0.5682 Acc: 0.8400\n",
      "\n",
      "Epoch 185/200\n",
      "----------\n",
      "train Loss: 0.0397 Acc: 1.0000\n",
      "val Loss: 0.5688 Acc: 0.8400\n",
      "\n",
      "Epoch 186/200\n",
      "----------\n",
      "train Loss: 0.0389 Acc: 1.0000\n",
      "val Loss: 0.5802 Acc: 0.8400\n",
      "\n",
      "Epoch 187/200\n",
      "----------\n",
      "train Loss: 0.0383 Acc: 1.0000\n",
      "val Loss: 0.6004 Acc: 0.8400\n",
      "\n",
      "Epoch 188/200\n",
      "----------\n",
      "train Loss: 0.0376 Acc: 1.0000\n",
      "val Loss: 0.5962 Acc: 0.8400\n",
      "\n",
      "Epoch 189/200\n",
      "----------\n",
      "train Loss: 0.0366 Acc: 1.0000\n",
      "val Loss: 0.5864 Acc: 0.8400\n",
      "\n",
      "Epoch 190/200\n",
      "----------\n",
      "train Loss: 0.0364 Acc: 1.0000\n",
      "val Loss: 0.5808 Acc: 0.8400\n",
      "\n",
      "Epoch 191/200\n",
      "----------\n",
      "train Loss: 0.0354 Acc: 1.0000\n",
      "val Loss: 0.5930 Acc: 0.8400\n",
      "\n",
      "Epoch 192/200\n",
      "----------\n",
      "train Loss: 0.0349 Acc: 1.0000\n",
      "val Loss: 0.6009 Acc: 0.8400\n",
      "\n",
      "Epoch 193/200\n",
      "----------\n",
      "train Loss: 0.0343 Acc: 1.0000\n",
      "val Loss: 0.5906 Acc: 0.8400\n",
      "\n",
      "Epoch 194/200\n",
      "----------\n",
      "train Loss: 0.0337 Acc: 1.0000\n",
      "val Loss: 0.5796 Acc: 0.8400\n",
      "\n",
      "Epoch 195/200\n",
      "----------\n",
      "train Loss: 0.0331 Acc: 1.0000\n",
      "val Loss: 0.5839 Acc: 0.8400\n",
      "\n",
      "Epoch 196/200\n",
      "----------\n",
      "train Loss: 0.0325 Acc: 1.0000\n",
      "val Loss: 0.5922 Acc: 0.8400\n",
      "\n",
      "Epoch 197/200\n",
      "----------\n",
      "train Loss: 0.0322 Acc: 1.0000\n",
      "val Loss: 0.5954 Acc: 0.8400\n",
      "\n",
      "Epoch 198/200\n",
      "----------\n",
      "train Loss: 0.0316 Acc: 1.0000\n",
      "val Loss: 0.5867 Acc: 0.8400\n",
      "\n",
      "Epoch 199/200\n",
      "----------\n",
      "train Loss: 0.0310 Acc: 1.0000\n",
      "val Loss: 0.5915 Acc: 0.8400\n",
      "\n",
      "Epoch 200/200\n",
      "----------\n",
      "train Loss: 0.0305 Acc: 1.0000\n",
      "val Loss: 0.5995 Acc: 0.8400\n",
      "\n",
      "Training complete in 1m 6s\n",
      "Best val Acc: 0.880000\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델 학습\n",
    "gru, gru_val_acc_history = train_model(gru, dataloaders_dict, criterion, num_epochs,\n",
    "                                       optimizer=optim.Adam(gru.parameters(), lr=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTpgK4Vm8RpP"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aST9oaz8RpQ"
   },
   "source": [
    "# <br>__4. Testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:53.328114Z",
     "iopub.status.busy": "2022-01-10T12:00:53.327847Z",
     "iopub.status.idle": "2022-01-10T12:00:53.335846Z",
     "shell.execute_reply": "2022-01-10T12:00:53.334998Z",
     "shell.execute_reply.started": "2022-01-10T12:00:53.328071Z"
    },
    "id": "WPw1E-nR8RpQ"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "    \n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # accuracy를 도출함\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:53.337918Z",
     "iopub.status.busy": "2022-01-10T12:00:53.337122Z",
     "iopub.status.idle": "2022-01-10T12:00:53.358191Z",
     "shell.execute_reply": "2022-01-10T12:00:53.357474Z",
     "shell.execute_reply.started": "2022-01-10T12:00:53.337855Z"
    },
    "id": "7yMR9zDC8RpQ",
    "outputId": "fe373d87-c482-412b-dbd1-d53e9a05de70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.6889\n"
     ]
    }
   ],
   "source": [
    "# Vanilla RNN 모델 검증 (Acc: 0.6889)\n",
    "test_model(rnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:53.359843Z",
     "iopub.status.busy": "2022-01-10T12:00:53.359589Z",
     "iopub.status.idle": "2022-01-10T12:00:53.376315Z",
     "shell.execute_reply": "2022-01-10T12:00:53.375602Z",
     "shell.execute_reply.started": "2022-01-10T12:00:53.359807Z"
    },
    "id": "p2eRzGlt8RpQ",
    "outputId": "ae1085d2-28a5-4a86-aa92-9613a2af4ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 검증 (Acc: 0.8222)\n",
    "# Vanilla RNN / LSTM / GRU 중 LSTM이 가장 높은 성능을 도출함\n",
    "test_model(lstm, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-10T12:00:53.377891Z",
     "iopub.status.busy": "2022-01-10T12:00:53.377540Z",
     "iopub.status.idle": "2022-01-10T12:00:53.393901Z",
     "shell.execute_reply": "2022-01-10T12:00:53.393186Z",
     "shell.execute_reply.started": "2022-01-10T12:00:53.377853Z"
    },
    "id": "R14rpxD98RpR",
    "outputId": "6d5a3c21-4d25-4c01-a32d-e70110a8c2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델 검증 (Acc: 0.7556)\n",
    "test_model(gru, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xyz47lRH8RpR"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_RNN_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
